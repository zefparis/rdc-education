import { Module } from './enhancedCourseConfig';

export const machineLearningCourse: Module = {
  id: '2',
  slug: 'machine-learning',
  title: 'Machine Learning Appliqu√©',
  description: 'Apprenez √† cr√©er des mod√®les pr√©dictifs avec scikit-learn : r√©gression, classification, clustering et √©valuation de mod√®les.',
  duration: '10 semaines',
  level: 'Interm√©diaire',
  icon: 'ü§ñ',
  color: 'from-green-500 to-blue-500',
  image: '/images/machine-learning.jpg',
  pdf: '/modules/machine-learning/README.md',
  notebook: '/modules/machine-learning/module-ml.ipynb',
  audio: '/audio/module-machine-learning.mp3',
  colabUrl: 'https://colab.research.google.com/github/ia-solution-rdc/ml/blob/main/module-ml.ipynb',
  objectives: [
    'Comprendre les concepts fondamentaux du Machine Learning',
    'Ma√Ætriser les algorithmes de r√©gression et classification',
    'Savoir pr√©parer et nettoyer les donn√©es pour l\'apprentissage',
    '√âvaluer correctement les performances des mod√®les',
    'Optimiser les hyperparam√®tres pour am√©liorer les r√©sultats',
    'D√©ployer des mod√®les en production'
  ],
  prerequisites: [
    'Connaissances solides en Python et Pandas (module Data Science)',
    'Notions de statistiques et probabilit√©s',
    'Compr√©hension de l\'alg√®bre lin√©aire de base',
    'Logique de programmation orient√©e objet'
  ],
  sections: [
    {
      id: 'ml-fundamentals',
      title: 'Concepts fondamentaux du Machine Learning',
      content: `
        <div class="course-section">
          <h3>Qu'est-ce que le Machine Learning ?</h3>
          <p>Le Machine Learning (ML) est un sous-domaine de l'Intelligence Artificielle qui permet aux ordinateurs d'apprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque t√¢che.</p>

          <h4>Les trois paradigmes d'apprentissage :</h4>
          <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0;">
            <div style="padding: 15px; background-color: #e6f3ff; border-radius: 8px; border-left: 4px solid #0066cc;">
              <h5 style="color: #0066cc; margin-top: 0;">üîç Apprentissage Supervis√©</h5>
              <p>Donn√©es √©tiquet√©es ‚Üí pr√©dictions</p>
              <ul>
                <li><strong>Classification :</strong> Cat√©goriser (spam/ham, maladie/pr√©sence)</li>
                <li><strong>R√©gression :</strong> Pr√©dire valeurs continues (prix, temp√©rature)</li>
              </ul>
            </div>
            <div style="padding: 15px; background-color: #fff3e6; border-radius: 8px; border-left: 4px solid #ff6600;">
              <h5 style="color: #ff6600; margin-top: 0;">üéØ Apprentissage Non Supervis√©</h5>
              <p>Donn√©es non √©tiquet√©es ‚Üí d√©couverte de patterns</p>
              <ul>
                <li><strong>Clustering :</strong> Grouper donn√©es similaires</li>
                <li><strong>R√©duction de dimension :</strong> Simplifier donn√©es</li>
              </ul>
            </div>
            <div style="padding: 15px; background-color: #e6ffe6; border-radius: 8px; border-left: 4px solid #00cc00;">
              <h5 style="color: #00cc00; margin-top: 0;">üéÆ Apprentissage par Renforcement</h5>
              <p>Agent apprend par interaction avec environnement</p>
              <ul>
                <li><strong>R√©compenses/P√©nalit√©s :</strong> Optimisation de strat√©gie</li>
                <li><strong>Applications :</strong> Jeux, robotique, trading</li>
              </ul>
            </div>
          </div>

          <h3>Le pipeline ML standard</h3>
          <ol>
            <li><strong>Collecte des donn√©es</strong> - Rassembler des donn√©es pertinentes</li>
            <li><strong>Pr√©paration des donn√©es</strong> - Nettoyage, transformation, ing√©nierie</li>
            <li><strong>Exploration des donn√©es</strong> - Analyse et visualisation</li>
            <li><strong>S√©lection du mod√®le</strong> - Choix de l'algorithme appropri√©</li>
            <li><strong>Entra√Ænement</strong> - Apprentissage sur donn√©es d'entra√Ænement</li>
            <li><strong>√âvaluation</strong> - Mesure des performances</li>
            <li><strong>Optimisation</strong> - Am√©lioration des r√©sultats</li>
            <li><strong>D√©ploiement</strong> - Mise en production</li>
          </ol>

          <div class="warning-box">
            <strong>‚ö†Ô∏è Important :</strong> Un mod√®le ne sera jamais meilleur que la qualit√© des donn√©es sur lesquelles il apprend.
          </div>
        </div>
      `,
      order: 1,
      estimatedTime: '2h 30min',
      codeExamples: [
        {
          title: 'Introduction √† scikit-learn',
          description: 'D√©couverte de la biblioth√®que ML principale en Python',
          code: `import numpy as np
import pandas as pd
from sklearn.datasets import load_iris, make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Chargement d'un dataset classique
print("üå∏ Dataset Iris (classification)")
iris = load_iris()
X, y = iris.data, iris.target

print(f"Dimensions: {X.shape}")
print(f"Features: {iris.feature_names}")
print(f"Classes: {iris.target_names}")
print(f"Distribution des classes: {np.bincount(y)}")

# 2. Cr√©ation d'un dataset synth√©tique
print("\\nüõ†Ô∏è Dataset synth√©tique (r√©gression)")
X_reg, y_reg = make_classification(
    n_samples=1000,
    n_features=10,
    n_informative=5,
    n_redundant=2,
    n_clusters_per_class=1,
    random_state=42
)

print(f"Dataset de r√©gression: {X_reg.shape}")
print(f"Valeurs cibles: min={y_reg.min()}, max={y_reg.max()}")

# 3. Division des donn√©es
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\\nüìä Division des donn√©es:")
print(f"Entra√Ænement: {X_train.shape}, Test: {X_test.shape}")

# 4. Pr√©paration des donn√©es
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Moyenne avant scaling: {X_train.mean():.3f}")
print(f"Moyenne apr√®s scaling: {X_train_scaled.mean():.3f}")

# 5. Premier mod√®le - R√©gression Logistique
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train_scaled, y_train)

# 6. Pr√©dictions et √©valuation
y_pred = model.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
print(f"\\nüéØ Pr√©cision du mod√®le: {accuracy:.3f}")

print("\\nüìã Rapport de classification:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

print("\\nüîç Matrice de confusion:")
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)`,
          language: 'python',
          explanation: 'Ce code montre le processus complet de Machine Learning : chargement des donn√©es, pr√©paration, entra√Ænement et √©valuation avec scikit-learn.'
        }
      ],
      exercises: [
        {
          id: 'ex-ml-pipeline',
          title: 'Impl√©mentez votre premier pipeline ML',
          description: 'Cr√©ez un pipeline complet avec pr√©processing et mod√®le pour le dataset Iris.',
          difficulty: 'facile',
          solution: `from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Cr√©ation du pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(random_state=42))
])

# Entra√Ænement
pipeline.fit(X_train, y_train)

# √âvaluation
accuracy = pipeline.score(X_test, y_test)
print(f"Pr√©cision avec pipeline: {accuracy:.3f}")

# Les pipelines garantissent que le preprocessing est appliqu√© correctement`
        }
      ],
      resources: [
        {
          title: 'Documentation scikit-learn',
          type: 'documentation',
          url: 'https://scikit-learn.org/stable/user_guide.html'
        },
        {
          title: 'Introduction au Machine Learning - Stanford',
          type: 'video',
          url: 'https://www.coursera.org/learn/machine-learning'
        }
      ]
    },
    {
      id: 'regression-algorithms',
      title: 'Algorithmes de r√©gression',
      content: `
        <div class="course-section">
          <h3>R√©gression lin√©aire et ses variantes</h3>
          <p>La r√©gression est utilis√©e pour pr√©dire des valeurs continues. Elle √©tablit une relation math√©matique entre variables ind√©pendantes et d√©pendante.</p>

          <h4>Types de r√©gression :</h4>
          <ul>
            <li><strong>R√©gression lin√©aire simple :</strong> Une variable ind√©pendante</li>
            <li><strong>R√©gression lin√©aire multiple :</strong> Plusieurs variables ind√©pendantes</li>
            <li><strong>R√©gression polynomiale :</strong> Variables avec termes non-lin√©aires</li>
            <li><strong>R√©gression r√©gularis√©e :</strong> Pr√©vention du surapprentissage</li>
          </ul>

          <h3>M√©triques d'√©valuation</h3>
          <ul>
            <li><strong>MAE (Mean Absolute Error) :</strong> Erreur absolue moyenne</li>
            <li><strong>MSE (Mean Squared Error) :</strong> Erreur quadratique moyenne</li>
            <li><strong>RMSE (Root Mean Squared Error) :</strong> Racine carr√©e du MSE</li>
            <li><strong>R¬≤ (Coefficient de d√©termination) :</strong> Qualit√© de l'ajustement</li>
          </ul>

          <div class="info-box">
            <strong>üí° Astuce :</strong> Choisissez la m√©trique selon votre probl√®me. R¬≤ pour expliquer la variance, MAE pour l'erreur absolue.
          </div>
        </div>
      `,
      order: 2,
      estimatedTime: '3h',
      codeExamples: [
        {
          title: 'R√©gression lin√©aire avec Boston Housing',
          description: 'Pr√©diction du prix des maisons avec diff√©rents algorithmes de r√©gression',
          code: `import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt

# Note: load_boston est d√©pr√©ci√©, utilisation d'un dataset alternatif
from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()
X, y = housing.data, housing.target

print("üè† Dataset California Housing")
print(f"Dimensions: {X.shape}")
print(f"Features: {housing.feature_names}")
print(f"Prix m√©dian: {y.mean():.2f} (en $100,000)")

# Division des donn√©es
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 1. R√©gression Lin√©aire Simple
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)

print("\\nüìà R√©gression Lin√©aire:")
print(f"R¬≤: {r2_score(y_test, lr_pred):.3f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, lr_pred)):.3f}")

# 2. R√©gression Ridge (L2 regularization)
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)
ridge_pred = ridge_model.predict(X_test)

print("\\nüèîÔ∏è R√©gression Ridge:")
print(f"R¬≤: {r2_score(y_test, ridge_pred):.3f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, ridge_pred)):.3f}")

# 3. R√©gression Lasso (L1 regularization)
lasso_model = Lasso(alpha=0.1)
lasso_model.fit(X_train, y_train)
lasso_pred = lasso_model.predict(X_test)

print("\\nü™¢ R√©gression Lasso:")
print(f"R¬≤: {r2_score(y_test, lasso_pred):.3f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, lasso_pred)):.3f}")

# 4. Random Forest (plus complexe)
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

print("\\nüå≥ Random Forest:")
print(f"R¬≤: {r2_score(y_test, rf_pred):.3f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, rf_pred)):.3f}")

# Comparaison des coefficients
print("\\nüìä Comparaison des mod√®les:")
models = ['Lin√©aire', 'Ridge', 'Lasso', 'Random Forest']
predictions = [lr_pred, ridge_pred, lasso_pred, rf_pred]

for name, pred in zip(models, predictions):
    mae = mean_absolute_error(y_test, pred)
    rmse = np.sqrt(mean_squared_error(y_test, pred))
    r2 = r2_score(y_test, pred)
    print(f"{name"15"} | MAE: {mae"6.3f"} | RMSE: {rmse"6.3f"} | R¬≤: {r2"6.3f"}")

# Validation crois√©e pour le meilleur mod√®le
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='r2')
print(f"\\n‚úÖ Validation crois√©e (R¬≤): {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")`,
          language: 'python',
          explanation: 'Ce code compare diff√©rents algorithmes de r√©gression sur un probl√®me r√©el de pr√©diction de prix immobiliers.'
        }
      ],
      exercises: [
        {
          id: 'ex-regression-diabetes',
          title: 'Pr√©diction du diab√®te avec r√©gression',
          description: 'Utilisez le dataset diabetes de scikit-learn pour pr√©dire la progression de la maladie.',
          difficulty: 'moyen',
          solution: `from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Chargement des donn√©es
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Division
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Mod√®le
model = LinearRegression()
model.fit(X_train, y_train)

# √âvaluation
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
print(f"R¬≤ sur les donn√©es de test: {r2:.3f}")
print(f"Equation: y = {model.intercept_:.2f} + {model.coef_[0]:.2f}x1 + ...")`
        }
      ]
    },
    {
      id: 'classification-algorithms',
      title: 'Algorithmes de classification',
      content: `
        <div class="course-section">
          <h3>Classification : pr√©dire des cat√©gories</h3>
          <p>La classification consiste √† pr√©dire la classe d'appartenance d'une observation bas√©e sur ses caract√©ristiques.</p>

          <h4>Principaux algorithmes :</h4>
          <ul>
            <li><strong>K-Nearest Neighbors (KNN) :</strong> Classification par proximit√©</li>
            <li><strong>Arbres de d√©cision :</strong> R√®gles de classification s√©quentielles</li>
            <li><strong>For√™ts al√©atoires :</strong> Ensemble d'arbres pour robustesse</li>
            <li><strong>Support Vector Machines (SVM) :</strong> S√©paration optimale des classes</li>
            <li><strong>Na√Øve Bayes :</strong> Probabilit√©s conditionnelles</li>
          </ul>

          <h3>Matrices de confusion et m√©triques</h3>
          <ul>
            <li><strong>Accuracy :</strong> Pourcentage de pr√©dictions correctes</li>
            <li><strong>Precision :</strong> Qualit√© des pr√©dictions positives</li>
            <li><strong>Recall (Sensibilit√©) :</strong> Capacit√© √† d√©tecter les positifs</li>
            <li><strong>F1-Score :</strong> Moyenne harmonique de precision et recall</li>
          </ul>
        </div>
      `,
      order: 3,
      estimatedTime: '3h 30min',
      codeExamples: [
        {
          title: 'Classification avec diff√©rents algorithmes',
          description: 'Comparaison de plusieurs algorithmes de classification sur le dataset des vins',
          code: `import numpy as np
import pandas as pd
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import matplotlib.pyplot as plt

# Chargement du dataset
wine = load_wine()
X, y = wine.data, wine.target

print("üç∑ Dataset Wine")
print(f"Dimensions: {X.shape}")
print(f"Classes: {wine.target_names}")
print(f"Distribution: {np.bincount(y)}")

# Division des donn√©es
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Pr√©paration des donn√©es
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Dictionnaire des mod√®les
models = {
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', random_state=42),
    'Naive Bayes': GaussianNB()
}

# Entra√Ænement et √©valuation de chaque mod√®le
results = {}

for name, model in models.items():
    # Entra√Ænement
    model.fit(X_train_scaled, y_train)

    # Pr√©dictions
    y_pred = model.predict(X_test_scaled)

    # M√©triques
    results[name] = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, average='weighted'),
        'recall': recall_score(y_test, y_pred, average='weighted'),
        'f1': f1_score(y_test, y_pred, average='weighted')
    }

    print(f"\\nüç∑ {name}:")
    print(f"Accuracy: {results[name]['accuracy']:.3f}")
    print(f"F1-Score: {results[name]['f1']:.3f}")

# Optimisation du meilleur mod√®le (Random Forest)
print("\\nüîç Optimisation du Random Forest...")
rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10]
}

rf_grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    rf_params,
    cv=5,
    scoring='f1_weighted',
    n_jobs=-1
)

rf_grid.fit(X_train_scaled, y_train)
best_rf = rf_grid.best_estimator_

# √âvaluation du mod√®le optimis√©
y_pred_best = best_rf.predict(X_test_scaled)
print("üèÜ Meilleur Random Forest:")
print(f"Meilleurs param√®tres: {rf_grid.best_params_}")
print(f"Accuracy: {accuracy_score(y_test, y_pred_best):.3f}")
print(f"F1-Score: {f1_score(y_test, y_pred_best, average='weighted'):.3f}")

print("\\nüìä Rapport d√©taill√© du meilleur mod√®le:")
print(classification_report(y_test, y_pred_best, target_names=wine.target_names))`,
          language: 'python',
          explanation: 'Cette comparaison montre comment diff√©rents algorithmes de classification performent sur un probl√®me de reconnaissance de vins.'
        }
      ],
      exercises: [
        {
          id: 'ex-classification-digits',
          title: 'Classification de chiffres manuscrits',
          description: 'Utilisez le dataset digits de scikit-learn pour cr√©er un mod√®le de reconnaissance de chiffres.',
          difficulty: 'moyen',
          solution: `from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# Chargement des donn√©es
digits = load_digits()
X, y = digits.data, digits.target

print(f"Dataset: {X.shape} images de {digits.data.shape[1]} pixels")

# Division
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Mod√®le
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# √âvaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Pr√©cision: {accuracy:.3f}")

# Matrice de confusion pour les chiffres difficiles (8 vs 3, etc.)
conf_matrix = confusion_matrix(y_test, y_pred)
print(f"Matrice de confusion:\\n{conf_matrix}")`
        }
      ]
    },
    {
      id: 'model-evaluation',
      title: '√âvaluation et validation des mod√®les',
      content: `
        <div class="course-section">
          <h3>L'importance de l'√©valuation rigoureuse</h3>
          <p>Un mod√®le qui semble performant peut √™tre trompeur. L'√©valuation rigoureuse est cruciale pour :</p>
          <ul>
            <li><strong>√âviter le surapprentissage</strong> (overfitting)</li>
            <li><strong>Estimer les performances r√©elles</strong> sur de nouvelles donn√©es</li>
            <li><strong>Comparer objectivement</strong> diff√©rents mod√®les</li>
            <li><strong>Identifier les points d'am√©lioration</strong></li>
          </ul>

          <h3>Techniques de validation</h3>
          <ul>
            <li><strong>Train/Test Split :</strong> Division simple mais peut manquer de stabilit√©</li>
            <li><strong>Cross-Validation :</strong> Utilisation de k plis pour une meilleure estimation</li>
            <li><strong>Validation crois√©e stratifi√©e :</strong> Maintien de la distribution des classes</li>
            <li><strong>Leave-One-Out CV :</strong> Pour petits datasets</li>
          </ul>

          <h3>Surapprentissage vs Sous-apprentissage</h3>
          <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
            <div style="padding: 15px; background-color: #ffe6e6; border-radius: 8px; border-left: 4px solid #cc0000;">
              <h5 style="color: #cc0000; margin-top: 0;">üî• Surapprentissage (Overfitting)</h5>
              <p>Mod√®le trop complexe qui m√©morise les donn√©es d'entra√Ænement</p>
              <ul>
                <li>Performance excellente sur train</li>
                <li>Performance m√©diocre sur test</li>
                <li>√âcart important entre train/test</li>
              </ul>
            </div>
            <div style="padding: 15px; background-color: #e6f3ff; border-radius: 8px; border-left: 4px solid #0066cc;">
              <h5 style="color: #0066cc; margin-top: 0;">‚ùÑÔ∏è Sous-apprentissage (Underfitting)</h5>
              <p>Mod√®le trop simple qui ne capture pas les patterns</p>
              <ul>
                <li>Performance m√©diocre sur train</li>
                <li>Performance similaire sur test</li>
                <li>Mod√®le √† complexifier</li>
              </ul>
            </div>
          </div>
        </div>
      `,
      order: 4,
      estimatedTime: '2h',
      codeExamples: [
        {
          title: 'Validation crois√©e et courbes d\'apprentissage',
          description: 'Techniques avanc√©es d\'√©valuation pour diagnostiquer les probl√®mes de mod√®le',
          code: `import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, validation_curve
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold

# Cr√©ation d'un dataset d√©s√©quilibr√© pour montrer l'importance de la stratification
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=10,
    n_redundant=10,
    n_classes=3,
    n_clusters_per_class=1,
    weights=[0.7, 0.2, 0.1],  # Classes d√©s√©quilibr√©es
    random_state=42
)

print(f"Distribution des classes: {np.bincount(y)}")

# Division classique vs stratifi√©e
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print(f"Distribution train (non-stratifi√©e): {np.bincount(y_train)}")
print(f"Distribution test (non-stratifi√©e): {np.bincount(y_test)}")

# Avec stratification
X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"Distribution train (stratifi√©e): {np.bincount(y_train_strat)}")
print(f"Distribution test (stratifi√©e): {np.bincount(y_test_strat)}")

# 1. Validation crois√©e classique
models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

for name, model in models.items():
    # Validation crois√©e simple
    cv_scores = cross_val_score(model, X_train_strat, y_train_strat, cv=5)
    print(f"\\nüìä {name} - CV Scores: {cv_scores}")
    print(f"CV Mean: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

    # Validation crois√©e stratifi√©e
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores_strat = cross_val_score(model, X_train_strat, y_train_strat, cv=skf)
    print(f"CV Stratifi√© Mean: {cv_scores_strat.mean():.3f} (+/- {cv_scores_strat.std() * 2:.3f})")

# 2. Courbes d'apprentissage pour diagnostiquer le surapprentissage
print("\\nüìà Analyse des courbes d'apprentissage...")

train_sizes = np.linspace(0.1, 1.0, 10)

for name, model in models.items():
    train_sizes_abs, train_scores, val_scores = learning_curve(
        model, X_train_strat, y_train_strat,
        train_sizes=train_sizes, cv=5, random_state=42
    )

    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(train_sizes_abs, train_scores.mean(axis=1), 'o-', label='Train')
    plt.plot(train_sizes_abs, val_scores.mean(axis=1), 'o-', label='Validation')
    plt.fill_between(train_sizes_abs,
                     train_scores.mean(axis=1) - train_scores.std(axis=1),
                     train_scores.mean(axis=1) + train_scores.std(axis=1),
                     alpha=0.1)
    plt.fill_between(train_sizes_abs,
                     val_scores.mean(axis=1) - val_scores.std(axis=1),
                     val_scores.mean(axis=1) + val_scores.std(axis=1),
                     alpha=0.1)
    plt.xlabel('Taille du training set')
    plt.ylabel('Score')
    plt.title(f'Courbes d\\'apprentissage - {name}')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    gap = train_scores.mean(axis=1) - val_scores.mean(axis=1)
    plt.plot(train_sizes_abs, gap, 'o-')
    plt.fill_between(train_sizes_abs,
                     gap - (train_scores.std(axis=1) + val_scores.std(axis=1)),
                     gap + (train_scores.std(axis=1) + val_scores.std(axis=1)),
                     alpha=0.1)
    plt.xlabel('Taille du training set')
    plt.ylabel('Gap Train-Validation')
    plt.title(f'Gap d\\'apprentissage - {name}')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

print("\\n‚úÖ Diagnostic: Un gap important indique du surapprentissage")`,
          language: 'python',
          explanation: 'Cette analyse approfondie montre comment diagnostiquer les probl√®mes de mod√®le avec des techniques de validation avanc√©es.'
        }
      ],
      exercises: [
        {
          id: 'ex-validation-cv',
          title: 'Impl√©mentez une validation crois√©e personnalis√©e',
          description: 'Cr√©ez une fonction qui effectue une validation crois√©e manuelle pour comprendre le processus.',
          difficulty: 'difficile',
          solution: `import numpy as np
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def custom_cross_validation(X, y, model, k=5):
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    scores = []

    for train_idx, val_idx in kf.split(X):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        model_fold = LogisticRegression(random_state=42, max_iter=1000)
        model_fold.fit(X_train, y_train)

        y_pred = model_fold.predict(X_val)
        score = accuracy_score(y_val, y_pred)
        scores.append(score)

    return scores

# Test de la fonction
scores = custom_cross_validation(X_train_strat, y_train_strat, LogisticRegression())
print(f"Scores CV personnalis√©e: {scores}")
print(f"Moyenne: {np.mean(scores):.3f} (+/- {np.std(scores) * 2:.3f})")`
        }
      ]
    }
  ],
  finalProject: {
    title: 'Projet Machine Learning End-to-End',
    description: 'D√©veloppez un mod√®le pr√©dictif complet sur un probl√®me r√©el de votre choix avec d√©ploiement d\'une API.',
    requirements: [
      'Collecte et pr√©paration d\'un dataset r√©el (minimum 1000 √©chantillons)',
      'Analyse exploratoire approfondie avec visualisations',
      'Comparaison d\'au moins 4 algorithmes diff√©rents',
      'Optimisation des hyperparam√®tres avec GridSearch ou RandomSearch',
      '√âvaluation rigoureuse avec validation crois√©e et m√©triques appropri√©es',
      'Interface web simple (Streamlit/Flask) pour utiliser le mod√®le',
      'Analyse des erreurs et interpr√©tabilit√© du mod√®le',
      'Documentation compl√®te du projet'
    ],
    deliverables: [
      'Notebook Jupyter d\'analyse et mod√©lisation',
      'Code source du mod√®le optimis√© et sauvegard√©',
      'Application web fonctionnelle avec interface utilisateur',
      'Rapport technique d√©taill√© (10-15 pages)',
      'Pr√©sentation orale de 15 minutes',
      'Code de d√©ploiement (Dockerfile, requirements.txt)',
      'Jeu de donn√©es nettoy√© et document√©'
    ]
  },
  resources: [
    {
      title: 'Scikit-learn documentation compl√®te',
      type: 'documentation',
      url: 'https://scikit-learn.org/stable/documentation.html'
    },
    {
      title: 'Machine Learning Yearning - Andrew Ng',
      type: 'article',
      url: 'https://www.mlyearning.org/'
    },
    {
      title: 'Kaggle Learn - Machine Learning',
      type: 'video',
      url: 'https://www.kaggle.com/learn/intro-to-machine-learning'
    },
    {
      title: 'Towards Data Science - ML Tutorials',
      type: 'article',
      url: 'https://towardsdatascience.com/tagged/machine-learning'
    }
  ],
  tags: ['machine-learning', 'scikit-learn', 'classification', 'regression', 'validation', 'python', 'data-science']
};
