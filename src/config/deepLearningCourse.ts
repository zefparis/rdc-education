import { Module } from './enhancedCourseConfig';

export const deepLearningCourse: Module = {
  id: '3',
  slug: 'deep-learning',
  title: 'Deep Learning avec TensorFlow',
  description: 'Plongez dans les r√©seaux de neurones profonds avec TensorFlow et Keras pour cr√©er des mod√®les d\'IA avanc√©s et r√©soudre des probl√®mes complexes.',
  duration: '12 semaines',
  level: 'Avanc√©',
  icon: 'üß†',
  color: 'from-purple-500 to-pink-500',
  image: '/images/deep-learning.jpg',
  pdf: '/modules/deep-learning/README.md',
  notebook: '/modules/deep-learning/module-dl.ipynb',
  audio: '/audio/module-deep-learning.mp3',
  colabUrl: 'https://colab.research.google.com/github/ia-solution-rdc/dl/blob/main/module-dl.ipynb',
  objectives: [
    'Comprendre l\'architecture et le fonctionnement des r√©seaux de neurones',
    'Ma√Ætriser TensorFlow et Keras pour le d√©veloppement de mod√®les',
    'Construire des r√©seaux de neurones pour diff√©rents types de probl√®mes',
    'Traiter et pr√©parer les donn√©es pour l\'apprentissage profond',
    '√âviter et diagnostiquer le surapprentissage',
    'Optimiser les performances des mod√®les',
    'D√©ployer des mod√®les de Deep Learning en production'
  ],
  prerequisites: [
    'Machine Learning solide (algorithmes, √©valuation)',
    'Programmation Python avanc√©e et orient√©e objet',
    'Math√©matiques : alg√®bre lin√©aire, calcul diff√©rentiel, probabilit√©s',
    'Exp√©rience avec NumPy et Pandas'
  ],
  sections: [
    {
      id: 'neural-networks-fundamentals',
      title: 'R√©seaux de neurones : concepts fondamentaux',
      content: `
        <div class="course-section">
          <h3>Introduction aux r√©seaux de neurones</h3>
          <p>Un r√©seau de neurones artificiel est inspir√© du fonctionnement du cerveau humain. Il s'agit d'un syst√®me de traitement de l'information compos√© de neurones interconnect√©s organis√©s en couches.</p>

          <h4>Architecture d'un r√©seau de neurones :</h4>
          <ul>
            <li><strong>Couche d'entr√©e :</strong> Re√ßoit les donn√©es d'origine</li>
            <li><strong>Couches cach√©es :</strong> Traitement interm√©diaire des informations</li>
            <li><strong>Couche de sortie :</strong> Produit le r√©sultat final</li>
            <li><strong>Connexions pond√©r√©es :</strong> Forces des liens entre neurones</li>
          </ul>

          <h3>Le perceptron : le neurone artificiel</h3>
          <p>Un perceptron est l'unit√© de base d'un r√©seau de neurones. Il calcule une combinaison lin√©aire de ses entr√©es, applique une fonction d'activation, et produit une sortie.</p>

          <h4>Fonctionnement du perceptron :</h4>
          <ol>
            <li><strong>Entr√©es pond√©r√©es :</strong> Chaque entr√©e est multipli√©e par un poids</li>
            <li><strong>Somme pond√©r√©e :</strong> Addition de toutes les entr√©es pond√©r√©es</li>
            <li><strong>Fonction d'activation :</strong> Transformation non-lin√©aire (ReLU, sigmoid, tanh)</li>
            <li><strong>Biais :</strong> Termes constants ajout√©s √† la somme</li>
          </ol>

          <h3>Fonctions d'activation courantes</h3>
          <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0;">
            <div style="padding: 15px; background-color: #f0f8ff; border-radius: 8px; border-left: 4px solid #1e90ff;">
              <h5 style="color: #1e90ff; margin-top: 0;">üî∑ Sigmoid</h5>
              <p>œÉ(x) = 1 / (1 + e^(-x))</p>
              <p><strong>Usage :</strong> Classification binaire, sortie de probabilit√©</p>
              <p><strong>Inconv√©nient :</strong> Vanishing gradient</p>
            </div>
            <div style="padding: 15px; background-color: #fff8dc; border-radius: 8px; border-left: 4px solid #ffa500;">
              <h5 style="color: #ffa500; margin-top: 0;">üü® Tanh</h5>
              <p>tanh(x) = 2œÉ(2x) - 1</p>
              <p><strong>Usage :</strong> Centrage autour de z√©ro</p>
              <p><strong>Avantage :</strong> Meilleur que sigmoid pour certains cas</p>
            </div>
            <div style="padding: 15px; background-color: #f0fff0; border-radius: 8px; border-left: 4px solid #32cd32;">
              <h5 style="color: #32cd32; margin-top: 0;">üü© ReLU</h5>
              <p>ReLU(x) = max(0, x)</p>
              <p><strong>Usage :</strong> R√©seaux profonds, tr√®s populaire</p>
              <p><strong>Avantages :</strong> Rapide, pas de vanishing gradient</p>
            </div>
          </div>

          <div class="info-box">
            <strong>üí° Insight :</strong> Le choix de la fonction d'activation d√©pend du probl√®me et de la position dans le r√©seau.
          </div>
        </div>
      `,
      order: 1,
      estimatedTime: '3h',
      codeExamples: [
        {
          title: 'Impl√©mentation d\'un perceptron simple',
          description: 'Cr√©ation et entra√Ænement d\'un perceptron pour la classification binaire',
          code: `import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Cr√©ation d'un dataset simple pour classification binaire
X, y = make_blobs(n_samples=1000, centers=2, n_features=2,
                  random_state=42, cluster_std=2)

# Visualisation des donn√©es
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', label='Classe 0', alpha=0.6)
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='red', label='Classe 1', alpha=0.6)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Dataset de classification binaire')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Pr√©paration des donn√©es
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Impl√©mentation du perceptron
class Perceptron:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape

        # Initialisation des poids
        self.weights = np.random.randn(n_features)
        self.bias = 0

        # Entra√Ænement
        for _ in range(self.n_iterations):
            for idx, x_i in enumerate(X):
                # Calcul de la sortie
                linear_output = np.dot(x_i, self.weights) + self.bias
                y_predicted = self._activation_function(linear_output)

                # Mise √† jour des poids (r√®gle du perceptron)
                update = self.learning_rate * (y[idx] - y_predicted)
                self.weights += update * x_i
                self.bias += update

    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        y_predicted = self._activation_function(linear_output)
        return y_predicted

    def _activation_function(self, x):
        return np.where(x >= 0, 1, 0)  # Fonction de seuil

# Entra√Ænement du perceptron
perceptron = Perceptron(learning_rate=0.1, n_iterations=100)
perceptron.fit(X_train, y_train)

# √âvaluation
predictions = perceptron.predict(X_test)
accuracy = np.mean(predictions == y_test)
print(f"Pr√©cision du perceptron: {accuracy:.3f}")

# Visualisation de la fronti√®re de d√©cision
def plot_decision_boundary(X, y, model):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', label='Classe 0')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='red', label='Classe 1')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Fronti√®re de d√©cision du perceptron')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

plot_decision_boundary(X_scaled, y, perceptron)`,
          language: 'python',
          explanation: 'Cette impl√©mentation montre le fonctionnement de base d\'un perceptron, l\'unit√© fondamentale des r√©seaux de neurones.'
        }
      ],
      exercises: [
        {
          id: 'ex-perceptron-logic',
          title: 'Impl√©mentez un perceptron pour apprendre la fonction logique AND.',
          description: 'Impl√©mentez un perceptron pour apprendre la fonction logique AND.',
          difficulty: 'moyen',
          solution: `# Dataset pour la fonction AND
X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_and = np.array([0, 0, 0, 1])

# Entra√Ænement du perceptron
perceptron_and = Perceptron(learning_rate=0.1, n_iterations=100)
perceptron_and.fit(X_and, y_and)

# Test
predictions_and = perceptron_and.predict(X_and)
print("Fonction AND - Pr√©dictions:", predictions_and)
print("Fonction AND - R√©el:", y_and)
print(f"Pr√©cision: {np.mean(predictions_and == y_and):.3f}")`
        }
      ],
      resources: [
        {
          title: 'Neural Networks and Deep Learning - Michael Nielsen',
          type: 'article',
          url: 'http://neuralnetworksanddeeplearning.com/'
        },
        {
          title: 'TensorFlow - R√©seaux de neurones',
          type: 'documentation',
          url: 'https://www.tensorflow.org/guide/keras/sequential_model'
        }
      ]
    },
    {
      id: 'tensorflow-keras',
      title: 'TensorFlow et Keras : d√©veloppement de mod√®les',
      content: `
        <div class="course-section">
          <h3>TensorFlow et Keras : l'√©cosyst√®me</h3>
          <p><strong>TensorFlow</strong> est la plateforme open-source de Google pour le Machine Learning, tandis que <strong>Keras</strong> est une API haut niveau qui simplifie la cr√©ation de mod√®les de Deep Learning.</p>

          <h4>Avantages de Keras :</h4>
          <ul>
            <li><strong>Simplicit√© :</strong> Interface intuitive et concise</li>
            <li><strong>Flexibilit√© :</strong> Support de TensorFlow, Theano, CNTK</li>
            <li><strong>Modularit√© :</strong> Assemblage facile de composants</li>
            <li><strong>Extensibilit√© :</strong> Couches personnalis√©es possibles</li>
          </ul>

          <h3>Architecture des mod√®les s√©quentiels</h3>
          <p>Le mod√®le s√©quentiel est le plus simple : une pile lin√©aire de couches.</p>

          <h4>Types de couches :</h4>
          <ul>
            <li><strong>Dense :</strong> Couches enti√®rement connect√©es</li>
            <li><strong>Conv2D :</strong> Convolution pour images</li>
            <li><strong>LSTM/GRU :</strong> R√©currence pour s√©quences</li>
            <li><strong>Dropout :</strong> R√©gularisation</li>
          </ul>

          <div class="warning-box">
            <strong>‚ö†Ô∏è Important :</strong> L'ordre des couches est crucial pour les performances et la convergence.
          </div>
        </div>
      `,
      order: 2,
      estimatedTime: '4h',
      codeExamples: [
        {
          title: 'Premier r√©seau de neurones avec Keras',
          description: 'Classification MNIST avec un r√©seau fully-connected simple',
          code: `import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

# Configuration pour la reproductibilit√©
np.random.seed(42)
tf.random.set_seed(42)

# 1. Chargement et pr√©paration du dataset MNIST
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()

# Normalisation et validation
X_train_full = X_train_full / 255.0
X_test = X_test / 255.0

X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

print(f"Entra√Ænement: {X_train.shape}")
print(f"Validation: {X_valid.shape}")
print(f"Test: {X_test.shape}")
print(f"Classes: {np.unique(y_train_full)}")

# 2. Cr√©ation du mod√®le s√©quentiel
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),  # Couche d'aplatissement
    keras.layers.Dense(300, activation="relu"),    # Couche cach√©e 1
    keras.layers.Dense(100, activation="relu"),    # Couche cach√©e 2
    keras.layers.Dense(10, activation="softmax")   # Couche de sortie
])

# 3. Compilation du mod√®le
model.compile(
    loss="sparse_categorical_crossentropy",  # Fonction de perte
    optimizer="adam",                        # Optimiseur
    metrics=["accuracy"]                     # M√©triques √† suivre
)

# 4. Affichage de l'architecture
model.summary()

# 5. Entra√Ænement du mod√®le
history = model.fit(
    X_train, y_train,
    epochs=20,
    validation_data=(X_valid, y_valid),
    batch_size=32,
    callbacks=[
        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
        keras.callbacks.ModelCheckpoint("best_mnist_model.h5", save_best_only=True)
    ]
)

# 6. √âvaluation sur le test set
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\\nüéØ Performance sur le test set:")
print(f"Loss: {test_loss:.4f}")
print(f"Accuracy: {test_acc:.4f}")

# 7. Analyse des courbes d'apprentissage
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('√âvolution de la pr√©cision')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('√âvolution de la perte')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 8. Pr√©dictions sur quelques exemples
predictions = model.predict(X_test[:5])
predicted_labels = np.argmax(predictions, axis=1)

print("\\nüîç Pr√©dictions sur les 5 premiers exemples:")
print(f"Pr√©dit: {predicted_labels}")
print(f"R√©el:   {y_test[:5]}")

# Affichage des images avec pr√©dictions
plt.figure(figsize=(10, 4))
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.imshow(X_test[i], cmap='gray')
    plt.title(f"Pr√©dit: {predicted_labels[i]}\\nR√©el: {y_test[i]}")
    plt.axis('off')
plt.tight_layout()
plt.show()`,
          language: 'python',
          explanation: 'Ce premier r√©seau de neurones avec Keras montre comment r√©soudre un probl√®me classique de reconnaissance de chiffres manuscrits (MNIST).'
        }
      ],
      exercises: [
        {
          id: 'ex-keras-fashion',
          title: 'Classification Fashion MNIST',
          description: 'Adaptez le code pr√©c√©dent pour classifier des v√™tements avec le dataset Fashion MNIST.',
          difficulty: 'moyen',
          solution: `# Adaptation pour Fashion MNIST
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

# M√™me pr√©processing
X_train_full = X_train_full / 255.0
X_test = X_test / 255.0
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# M√™me mod√®le mais avec plus de capacit√©
model_fashion = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(512, activation="relu"),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(256, activation="relu"),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation="softmax")
])

model_fashion.compile(loss="sparse_categorical_crossentropy",
                      optimizer="adam",
                      metrics=["accuracy"])

# Entra√Ænement plus long
history_fashion = model_fashion.fit(X_train, y_train, epochs=30,
                                    validation_data=(X_valid, y_valid))

# √âvaluation
test_loss, test_acc = model_fashion.evaluate(X_test, y_test)
print(f"Fashion MNIST - Accuracy: {test_acc:.3f}")`
        }
      ]
    },
    {
      id: 'convolutional-neural-networks',
      title: 'R√©seaux de neurones convolutionnels (CNN)',
      content: `
        <div class="course-section">
          <h3>Convolution : extraction de features</h3>
          <p>Les CNN sont sp√©cialement con√ßus pour traiter des donn√©es avec structure spatiale comme les images. Ils utilisent la convolution pour extraire automatiquement des caract√©ristiques hi√©rarchiques.</p>

          <h4>Couches convolutionnelles :</h4>
          <ul>
            <li><strong>Filtres/Kernels :</strong> Matrices apprises qui d√©tectent des patterns</li>
            <li><strong>Stride :</strong> Pas de d√©placement du filtre</li>
            <li><strong>Padding :</strong> Gestion des bordures de l'image</li>
            <li><strong>Pooling :</strong> R√©duction de dimension et g√©n√©ralisation</li>
          </ul>

          <h3>Architecture CNN typique</h3>
          <ol>
            <li><strong>Couches Conv2D :</strong> Extraction de features locales</li>
            <li><strong>Couches Pooling :</strong> R√©duction de complexit√©</li>
            <li><strong>Couches Dense :</strong> Classification finale</li>
            <li><strong>Dropout :</strong> Pr√©vention du surapprentissage</li>
          </ol>

          <h3>Transfer Learning</h3>
          <p>Utiliser des mod√®les pr√©-entra√Æn√©s (VGG16, ResNet, etc.) comme base et les adapter √† votre probl√®me sp√©cifique.</p>

          <div style="background-color: #e6f3ff; padding: 15px; border-radius: 8px; border-left: 4px solid #0066cc; margin: 20px 0;">
            <h5 style="color: #0066cc; margin-top: 0;">üîÑ Avantages du Transfer Learning</h5>
            <ul>
              <li><strong>Gain de temps :</strong> Pas besoin d'entra√Æner de z√©ro</li>
              <li><strong>Meilleures performances :</strong> Mod√®les pr√©-entra√Æn√©s sur datasets massifs</li>
              <li><strong>Moins de donn√©es n√©cessaires :</strong> Fine-tuning sur petit dataset</li>
            </ul>
          </div>
        </div>
      `,
      order: 3,
      estimatedTime: '5h',
      codeExamples: [
        {
          title: 'CNN pour classification d\'images CIFAR-10',
          description: 'Cr√©ation d\'un r√©seau convolutionnel pour reconna√Ætre des objets dans des images',
          code: `import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

# 1. Chargement du dataset CIFAR-10
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()

# Normalisation
X_train_full = X_train_full / 255.0
X_test = X_test / 255.0

X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# Classes du dataset
class_names = ['avion', 'auto', 'oiseau', 'chat', 'cerf', 'chien', 'grenouille', 'cheval', 'bateau', 'camion']

print(f"Entra√Ænement: {X_train.shape}")
print(f"Test: {X_test.shape}")
print(f"Classes: {len(class_names)}")

# 2. Cr√©ation du mod√®le CNN
model = keras.models.Sequential([
    # Bloc convolutionnel 1
    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=[32, 32, 3]),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.25),

    # Bloc convolutionnel 2
    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.25),

    # Bloc convolutionnel 3
    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.25),

    # Classification
    keras.layers.Flatten(),
    keras.layers.Dense(512, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(10, activation='softmax')
])

# 3. Compilation
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

# 4. Callbacks pour am√©liorer l'entra√Ænement
callbacks = [
    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.00001),
    keras.callbacks.ModelCheckpoint('best_cifar_model.h5', save_best_only=True)
]

# 5. Entra√Ænement
history = model.fit(
    X_train, y_train,
    epochs=50,
    validation_data=(X_valid, y_valid),
    batch_size=64,
    callbacks=callbacks,
    verbose=1
)

# 6. √âvaluation
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\\nüéØ Performance sur CIFAR-10:")
print(f"Loss: {test_loss:.4f}")
print(f"Accuracy: {test_acc:.4f}")

# 7. Visualisation des pr√©dictions
def plot_predictions(X, y_true, y_pred, class_names, n_samples=5):
    plt.figure(figsize=(15, 6))

    indices = np.random.choice(len(X), n_samples, replace=False)

    for i, idx in enumerate(indices):
        plt.subplot(2, n_samples, i + 1)
        plt.imshow(X[idx])
        plt.title(f"R: {class_names[y_true[idx][0]]}\\nP: {class_names[y_pred[idx]]}")
        plt.axis('off')

        plt.subplot(2, n_samples, i + n_samples + 1)
        # Cr√©ation d'un heatmap simple des activations
        plt.imshow(np.random.rand(8, 8), cmap='hot', interpolation='nearest')
        plt.title('Activations (simul√©)')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# Pr√©dictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

plot_predictions(X_test, y_test, y_pred_classes, class_names)`,
          language: 'python',
          explanation: 'Ce CNN montre comment traiter des images couleur complexes avec une architecture convolutionnelle moderne utilisant BatchNormalization et Dropout.'
        }
      ],
      exercises: [
        {
          id: 'ex-cnn-custom',
          title: 'CNN personnalis√© pour petits datasets',
          description: 'Cr√©ez un CNN simple pour le dataset MNIST avec data augmentation.',
          difficulty: 'difficile',
          solution: `from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Data augmentation pour √©viter le surapprentissage
datagen = ImageDataGenerator(
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    fill_mode='nearest'
)

# Cr√©ation du mod√®le CNN simple
model_simple = keras.models.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=[28, 28, 1]),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model_simple.compile(loss='sparse_categorical_crossentropy',
                     optimizer='adam',
                     metrics=['accuracy'])

# Entra√Ænement avec data augmentation
history = model_simple.fit(
    datagen.flow(X_train.reshape(-1, 28, 28, 1), y_train, batch_size=32),
    epochs=20,
    validation_data=(X_valid.reshape(-1, 28, 28, 1), y_valid)
)

# √âvaluation
test_loss, test_acc = model_simple.evaluate(X_test.reshape(-1, 28, 28, 1), y_test)
print(f"CNN simple - Accuracy: {test_acc:.3f}")`
        }
      ]
    },
    {
      id: 'recurrent-neural-networks',
      title: 'R√©seaux de neurones r√©currents (RNN)',
      content: `
        <div class="course-section">
          <h3>Traitement des donn√©es s√©quentielles</h3>
          <p>Les RNN sont con√ßus pour traiter des donn√©es s√©quentielles o√π l'ordre importe : s√©ries temporelles, texte, audio, vid√©os.</p>

          <h4>Principe de r√©currence :</h4>
          <ul>
            <li><strong>M√©moire :</strong> Les RNN maintiennent un √©tat interne (hidden state)</li>
            <li><strong>D√©pendance temporelle :</strong> Chaque pr√©diction d√©pend des pr√©c√©dentes</li>
            <li><strong>Partage des poids :</strong> M√™me fonction appliqu√©e √† chaque pas de temps</li>
          </ul>

          <h3>Probl√®mes des RNN classiques</h3>
          <ul>
            <li><strong>Vanishing gradient :</strong> Gradients deviennent tr√®s petits</li>
            <li><strong>Exploding gradient :</strong> Gradients deviennent tr√®s grands</li>
            <li><strong>D√©pendances √† long terme :</strong> Difficult√© √† retenir l'information</li>
          </ul>

          <h3>LSTM et GRU : solutions avanc√©es</h3>
          <p><strong>LSTM (Long Short-Term Memory)</strong> et <strong>GRU (Gated Recurrent Unit)</strong> r√©solvent ces probl√®mes avec des m√©canismes de gating sophistiqu√©s.</p>

          <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
            <div style="padding: 15px; background-color: #fff3cd; border-radius: 8px; border-left: 4px solid #ffc107;">
              <h5 style="color: #856404; margin-top: 0;">üß† LSTM</h5>
              <p>Plus complexe mais plus puissant</p>
              <ul>
                <li>Cell state s√©par√©</li>
                <li>3 gates (forget, input, output)</li>
                <li>Meilleur pour s√©quences longues</li>
              </ul>
            </div>
            <div style="padding: 15px; background-color: #d4edda; border-radius: 8px; border-left: 4px solid #28a745;">
              <h5 style="color: #155724; margin-top: 0;">‚ö° GRU</h5>
              <p>Version simplifi√©e mais efficace</p>
              <ul>
                <li>2 gates seulement</li>
                <li>Plus rapide √† entra√Æner</li>
                <li>Performance similaire au LSTM</li>
              </ul>
            </div>
          </div>
        </div>
      `,
      order: 4,
      estimatedTime: '4h',
      codeExamples: [
        {
          title: 'Pr√©diction de s√©ries temporelles avec LSTM',
          description: 'Utilisation d\'un LSTM pour pr√©dire la valeur future d\'une s√©rie temporelle',
          code: `import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow import keras

# 1. G√©n√©ration d'une s√©rie temporelle sinuso√Ødale avec tendance
np.random.seed(42)
n_points = 1000
t = np.linspace(0, 4*np.pi, n_points)

# S√©rie avec tendance et bruit
trend = 0.5 * t
seasonal = 2 * np.sin(2*t) + np.sin(5*t)
noise = 0.3 * np.random.randn(n_points)
time_series = trend + seasonal + noise

# Pr√©paration des donn√©es pour le RNN
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:(i + seq_length)])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

sequence_length = 50
X, y = create_sequences(time_series, sequence_length)

# Division train/test
train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

print(f"Train: {X_train.shape}, Test: {X_test.shape}")

# Reshape pour LSTM [samples, time_steps, features]
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# 2. Cr√©ation du mod√®le LSTM
model = keras.models.Sequential([
    keras.layers.LSTM(50, activation='relu', return_sequences=True, input_shape=(sequence_length, 1)),
    keras.layers.LSTM(50, activation='relu'),
    keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')

# 3. Entra√Ænement
history = model.fit(X_train, y_train, epochs=50, batch_size=32,
                    validation_split=0.1, verbose=1)

# 4. √âvaluation
y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f"\\nüìä Performance du mod√®le LSTM:")
print(f"RMSE: {rmse:.4f}")

# 5. Visualisation des pr√©dictions
plt.figure(figsize=(15, 8))

# S√©rie compl√®te
plt.subplot(2, 1, 1)
plt.plot(range(len(time_series)), time_series, label='S√©rie r√©elle', alpha=0.7)
plt.axvline(x=train_size + sequence_length, color='red', linestyle='--', label='D√©but pr√©dictions')
plt.xlabel('Temps')
plt.ylabel('Valeur')
plt.title('S√©rie temporelle compl√®te avec pr√©dictions LSTM')
plt.legend()
plt.grid(True, alpha=0.3)

# Zoom sur les pr√©dictions
plt.subplot(2, 1, 2)
test_indices = range(train_size + sequence_length, len(time_series))
plt.plot(test_indices, time_series[train_size + sequence_length:], label='Valeurs r√©elles', color='blue')
plt.plot(test_indices, y_pred.flatten(), label='Pr√©dictions LSTM', color='red', linestyle='--')
plt.xlabel('Temps')
plt.ylabel('Valeur')
plt.title('Pr√©dictions vs R√©el (zone de test)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 6. Pr√©diction future (forecasting)
def predict_future(model, last_sequence, n_steps):
    predictions = []
    current_sequence = last_sequence.copy()

    for _ in range(n_steps):
        # Pr√©diction du prochain point
        next_pred = model.predict(current_sequence.reshape(1, sequence_length, 1))
        predictions.append(next_pred[0, 0])

        # Mise √† jour de la s√©quence
        current_sequence = np.roll(current_sequence, -1)
        current_sequence[-1] = next_pred[0, 0]

    return np.array(predictions)

# Pr√©diction de 50 points futurs
last_sequence = time_series[-sequence_length:]
future_predictions = predict_future(model, last_sequence, 50)

# Visualisation avec pr√©dictions futures
plt.figure(figsize=(12, 6))
plt.plot(range(len(time_series)), time_series, label='Donn√©es historiques', color='blue')
plt.plot(range(len(time_series), len(time_series) + 50), future_predictions,
         label='Pr√©dictions futures', color='red', linestyle='--')
plt.axvline(x=len(time_series), color='red', linestyle='--', alpha=0.7)
plt.xlabel('Temps')
plt.ylabel('Valeur')
plt.title('Pr√©dictions futures avec le mod√®le LSTM')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()`,
          language: 'python',
          explanation: 'Ce mod√®le LSTM montre comment pr√©dire l\'√©volution future d\'une s√©rie temporelle en apprenant les patterns pass√©s.'
        }
      ],
      exercises: [
        {
          id: 'ex-lstm-text',
          title: 'G√©n√©ration de texte avec LSTM',
          description: 'Cr√©ez un mod√®le LSTM simple pour g√©n√©rer du texte √† partir d\'un corpus d\'entra√Ænement.',
          difficulty: 'difficile',
          solution: `from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Texte d'exemple simple
text = "Bonjour le monde. Bonjour l'intelligence artificielle. Bonjour le deep learning."

# Tokenization
tokenizer = Tokenizer(char_level=True)  # Caract√®res plut√¥t que mots pour la simplicit√©
tokenizer.fit_on_texts([text])

# Conversion en s√©quences
sequences = []
for i in range(1, len(text)):
    seq = text[:i+1]
    sequences.append(seq)

# Pr√©paration des donn√©es
max_sequence_len = max([len(seq) for seq in sequences])
vocab_size = len(tokenizer.word_index) + 1

# Cr√©ation des s√©quences d'entr√©e/sortie
X = []
y = []
for seq in sequences:
    encoded = tokenizer.texts_to_sequences([seq])[0]
    X.append(encoded[:-1])
    y.append(encoded[-1])

X = pad_sequences(X, maxlen=max_sequence_len-1, padding='pre')
y = keras.utils.to_categorical(y, num_classes=vocab_size)

# Mod√®le LSTM simple
model_text = keras.models.Sequential([
    keras.layers.Embedding(vocab_size, 50, input_length=max_sequence_len-1),
    keras.layers.LSTM(100),
    keras.layers.Dense(vocab_size, activation='softmax')
])

model_text.compile(loss='categorical_crossentropy', optimizer='adam')

# Entra√Ænement rapide
model_text.fit(X, y, epochs=100, verbose=0)

print("Mod√®le de g√©n√©ration de texte entra√Æn√©!")
print(f"Vocabulaire: {vocab_size} caract√®res")`
        }
      ]
    }
  ],
  finalProject: {
    title: 'Projet Deep Learning Avanc√© : Application compl√®te',
    description: 'D√©veloppez une application de Deep Learning compl√®te avec interface utilisateur pour un probl√®me r√©el de votre choix.',
    requirements: [
      'Choix d\'un domaine sp√©cifique (vision, NLP, s√©ries temporelles, audio)',
      'Collecte et pr√©paration d\'un dataset appropri√© (minimum 10,000 √©chantillons)',
      'Conception et impl√©mentation d\'une architecture de r√©seau adapt√©e',
      'Entra√Ænement pouss√© avec optimisation des hyperparam√®tres',
      '√âvaluation rigoureuse avec m√©triques appropri√©es et analyse d\'erreurs',
      'Interface web moderne (React/Streamlit) pour d√©monstration',
      'Comparaison avec d\'autres approches (ML classique, autres architectures)',
      'Analyse de l\'interpr√©tabilit√© et des limitations du mod√®le',
      'Optimisation pour le d√©ploiement (quantization, pruning)',
      'Documentation compl√®te et guide d\'utilisation'
    ],
    deliverables: [
      'Code source complet avec architecture modulaire',
      'Notebook d\'exp√©rimentation et d\'analyse',
      'Application web fonctionnelle avec interface intuitive',
      'Mod√®le entra√Æn√© optimis√© et sauvegard√©',
      'Rapport technique d√©taill√© (15-20 pages)',
      'Pr√©sentation orale de 20 minutes avec d√©monstration',
      'Code de d√©ploiement (Docker, scripts d\'installation)',
      'Jeu de donn√©es nettoy√© et document√©',
      'Guide d\'utilisateur et tutoriels',
      '√âvaluation des performances et benchmarks'
    ]
  },
  resources: [
    {
      title: 'Deep Learning Specialization - Coursera (Andrew Ng)',
      type: 'video',
      url: 'https://www.coursera.org/specializations/deep-learning'
    },
    {
      title: 'TensorFlow Documentation compl√®te',
      type: 'documentation',
      url: 'https://www.tensorflow.org/guide'
    },
    {
      title: 'CS231n - Convolutional Neural Networks',
      type: 'video',
      url: 'https://cs231n.github.io/'
    },
    {
      title: 'Attention Is All You Need - Paper fondateur',
      type: 'article',
      url: 'https://arxiv.org/abs/1706.03762'
    },
    {
      title: 'Papers with Code - √âtat de l\'art',
      type: 'article',
      url: 'https://paperswithcode.com/'
    }
  ],
  tags: ['deep-learning', 'tensorflow', 'keras', 'neural-networks', 'cnn', 'rnn', 'lstm', 'python', 'ai']
};
