# Module Deep Learning - Support de cours
## Partie 1 : Introduction et Fondamentaux

**Plateforme IA-Solution RDC**  
**Niveau :** Interm√©diaire | **Dur√©e :** 10 semaines

---

## üìã Table des mati√®res compl√®te

1. Introduction au Deep Learning
2. R√©seaux de neurones artificiels
3. Entra√Ænement d'un r√©seau
4. TensorFlow et PyTorch
5. Probl√®mes et solutions
6. Projet final

---

# Chapitre 1 : Introduction au Deep Learning

## 1.1 Qu'est-ce que le Deep Learning ?

Le **Deep Learning** (apprentissage profond) est une branche du Machine Learning qui utilise des **r√©seaux de neurones artificiels** avec plusieurs couches pour apprendre des repr√©sentations complexes des donn√©es.

### Analogie simple

Imaginez que vous apprenez √† un enfant √† reconna√Ætre des fruits au march√© de Kinshasa :

**ML Classique :**
- Vous donnez des r√®gles : "Si c'est jaune et allong√©, c'est une banane"
- Vous devez penser √† toutes les r√®gles

**Deep Learning :**
- Vous montrez des milliers de photos
- Il apprend lui-m√™me les caract√©ristiques
- Il peut reconna√Ætre des fruits jamais vus

## 1.2 Diff√©rences ML vs Deep Learning

| Aspect | ML Classique | Deep Learning |
|--------|--------------|---------------|
| **Features** | Manuelles | Automatiques |
| **Donn√©es** | 100-10K | 10K-millions |
| **Calcul** | CPU | GPU |
| **Temps** | Minutes-heures | Heures-jours |
| **Interpr√©tabilit√©** | √âlev√©e | Faible |
| **Performance** | Bonne | Excellente |

## 1.3 Applications en RDC

### üè• Sant√©
- D√©tection du paludisme sur lames
- Analyse de radiographies
- Pr√©diction d'√©pid√©mies

### üåæ Agriculture
- D√©tection maladies du manioc
- Estimation des rendements
- Classification des sols

### üí∞ Commerce
- Reconnaissance de produits
- D√©tection de fraudes
- Recommandations

### üìö √âducation
- Correction automatique
- Chatbots √©ducatifs
- Analyse de performances

---

# Chapitre 2 : R√©seaux de neurones

## 2.1 Le neurone artificiel

### Composants
1. **Entr√©es (x)** : Donn√©es
2. **Poids (w)** : Importance
3. **Biais (b)** : Ajustement
4. **Activation (f)** : Transformation

### Formule
```
z = (x‚ÇÅ√ów‚ÇÅ) + (x‚ÇÇ√ów‚ÇÇ) + ... + b
y = f(z)
```

## 2.2 Fonctions d'activation

### Sigmoid
- **Formule :** œÉ(z) = 1/(1+e‚Åª·∂ª)
- **Sortie :** [0, 1]
- **Usage :** Classification binaire

### ReLU
- **Formule :** f(z) = max(0, z)
- **Sortie :** [0, ‚àû)
- **Usage :** Couches cach√©es (recommand√©)

### Softmax
- **Formule :** e·∂ª‚Å±/Œ£e·∂ª ≤
- **Sortie :** Probabilit√©s (somme=1)
- **Usage :** Classification multi-classes

## 2.3 Architecture

```
Entr√©e ‚Üí Couches cach√©es ‚Üí Sortie
  (5)      (32‚Üí16‚Üí8)        (3)
```

**Exemple : Classification de fruits**
- Entr√©e : 5 features (longueur, largeur, poids, couleur, texture)
- Cach√©es : 3 couches (32, 16, 8 neurones)
- Sortie : 3 classes (banane, mangue, ananas)

---

# Chapitre 3 : Entra√Ænement

## 3.1 Fonction de perte

Mesure l'erreur entre pr√©diction et r√©alit√©.

### Types principaux

| Fonction | Usage | Formule |
|----------|-------|---------|
| **Binary Crossentropy** | 2 classes | -[y log(≈∑) + (1-y) log(1-≈∑)] |
| **Categorical Crossentropy** | N classes | -Œ£ y·µ¢ log(≈∑·µ¢) |
| **MSE** | R√©gression | (y - ≈∑)¬≤ |

## 3.2 Backpropagation

Algorithme pour ajuster les poids.

### √âtapes
1. **Forward** : Calculer pr√©diction
2. **Erreur** : Comparer avec v√©rit√©
3. **Backward** : Propager erreur
4. **Update** : Ajuster poids

### Analogie
Comme apprendre √† lancer une balle :
1. Vous lancez
2. Vous ratez
3. Vous analysez
4. Vous ajustez
5. Vous relancez

## 3.3 Optimiseurs

### Adam (Recommand√©)
- Adapte le learning rate
- Convergence rapide
- Peu de r√©glages

### SGD
- Simple
- Plus lent
- N√©cessite r√©glages

## 3.4 Hyperparam√®tres

### Learning Rate
- **Trop petit** : Lent
- **Trop grand** : Diverge
- **Optimal** : 0.001-0.01

### √âpoques
- **Trop peu** : Sous-apprentissage
- **Trop** : Surapprentissage
- **Optimal** : 50-200

### Batch Size
- **Petit (32)** : Rapide, bruit√©
- **Grand (128)** : Lent, stable

---

# Chapitre 4 : TensorFlow

## 4.1 Introduction

Biblioth√®que Google pour Deep Learning.

### Avantages
- ‚úÖ Populaire
- ‚úÖ Keras int√©gr√©
- ‚úÖ Production
- ‚úÖ Mobile

### Installation
```bash
pip install tensorflow
```

## 4.2 Exemple basique

```python
import tensorflow as tf
from tensorflow import keras

# Cr√©er mod√®le
model = keras.Sequential([
    keras.layers.Dense(16, activation='relu', input_shape=(4,)),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

# Compiler
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Entra√Æner
model.fit(X_train, y_train, epochs=50)

# Pr√©dire
predictions = model.predict(X_test)
```

---

# Chapitre 5 : Probl√®mes courants

## 5.1 Surapprentissage

### Signes
- ‚úÖ Train excellent
- ‚ùå Test mauvais
- üìà √âcart croissant

### Solutions

#### 1. Dropout
```python
keras.layers.Dropout(0.3)  # 30%
```

#### 2. R√©gularisation
```python
kernel_regularizer=regularizers.l2(0.01)
```

#### 3. Early Stopping
```python
EarlyStopping(patience=10)
```

#### 4. Plus de donn√©es
- Collecter plus
- Data augmentation

## 5.2 Sous-apprentissage

### Signes
- ‚ùå Train mauvais
- ‚ùå Test mauvais

### Solutions
- Mod√®le plus complexe
- Plus d'√©poques
- Meilleures features

---

# Chapitre 6 : Projet - Fruits

## 6.1 Objectif

Classifier 3 fruits congolais :
- üçå Banane
- ü•≠ Mangue
- üçç Ananas

## 6.2 Features

1. Longueur (cm)
2. Largeur (cm)
3. Poids (g)
4. Couleur (0-1)
5. Texture (0-1)

## 6.3 Architecture

```
Input (5) ‚Üí Dense(32) ‚Üí Dropout(0.3) 
‚Üí Dense(16) ‚Üí Dropout(0.2) 
‚Üí Dense(8) ‚Üí Output(3)
```

## 6.4 Code complet

```python
# Donn√©es
X = np.array([...])  # 300 exemples
y = np.array([...])  # Labels 0,1,2

# Normaliser
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# One-hot encoding
y_cat = keras.utils.to_categorical(y, 3)

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_cat, test_size=0.2
)

# Mod√®le
model = keras.Sequential([
    keras.layers.Dense(32, activation='relu', input_shape=(5,)),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(16, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.Dense(3, activation='softmax')
])

# Compiler
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Entra√Æner
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=100,
    callbacks=[EarlyStopping(patience=20)]
)

# √âvaluer
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Pr√©cision: {test_acc*100:.2f}%")
```

## 6.5 R√©sultats attendus

- **Pr√©cision** : > 90%
- **Temps** : 2-3 minutes
- **Param√®tres** : ~900

---

# Exercices pratiques

## Exercice 1 : Perceptron ‚≠ê
Cr√©er un neurone pour pr√©dire r√©ussite √©tudiant.

## Exercice 2 : Learning Rate ‚≠ê‚≠ê
Tester diff√©rents learning rates.

## Exercice 3 : Architecture ‚≠ê‚≠ê
Cr√©er un r√©seau pour pr√©dire rendement agricole.

## Exercice 4 : Surapprentissage ‚≠ê‚≠ê‚≠ê
Comparer mod√®le avec/sans dropout.

## Exercice 5 : Am√©lioration ‚≠ê‚≠ê‚≠ê
Am√©liorer le mod√®le de fruits.

---

# Ressources

## Cours en ligne
- Deep Learning Specialization (Coursera)
- Fast.ai
- TensorFlow Tutorials

## Livres
- "Deep Learning" - Goodfellow
- "Hands-On ML" - G√©ron

## Communaut√©s
- Kaggle
- Papers With Code
- Reddit r/MachineLearning

---

**F√©licitations ! Module termin√© ! üéâ**

*Continuez √† pratiquer et explorer !*
