{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Deep Learning - Formation Complète\n",
    "\n",
    "## Plateforme IA-Solution RDC\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Objectifs du module\n",
    "\n",
    "À la fin de ce module, vous serez capable de :\n",
    "- Comprendre ce qu'est le Deep Learning et ses différences avec le ML classique\n",
    "- Maîtriser les réseaux de neurones artificiels (architecture, fonctionnement)\n",
    "- Implémenter des réseaux avec TensorFlow et PyTorch\n",
    "- Résoudre des problèmes de classification et régression\n",
    "- Appliquer le Deep Learning à des cas concrets en RDC\n",
    "\n",
    "**Niveau :** Intermédiaire  \n",
    "**Prérequis :** Python, bases de Machine Learning  \n",
    "**Durée :** 10 semaines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Table des matières\n",
    "\n",
    "1. [Introduction au Deep Learning](#chapitre-1)\n",
    "2. [Réseaux de neurones artificiels](#chapitre-2)\n",
    "3. [Entraînement d'un réseau](#chapitre-3)\n",
    "4. [TensorFlow et PyTorch](#chapitre-4)\n",
    "5. [Problèmes et solutions](#chapitre-5)\n",
    "6. [Projet final : Classification de fruits](#chapitre-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 1 : Introduction au Deep Learning <a id=\"chapitre-1\"></a>\n",
    "\n",
    "## 1.1 Qu'est-ce que le Deep Learning ?\n",
    "\n",
    "Le **Deep Learning** (apprentissage profond) est une branche du Machine Learning qui utilise des **réseaux de neurones artificiels** avec plusieurs couches pour apprendre des représentations complexes des données.\n",
    "\n",
    "### Analogie simple\n",
    "\n",
    "Imaginez que vous apprenez à un enfant à reconnaître des fruits :\n",
    "- **Machine Learning classique** : Vous lui donnez des règles précises (\"si c'est jaune et allongé, c'est une banane\")\n",
    "- **Deep Learning** : Vous lui montrez des milliers d'images de fruits, et il apprend lui-même les caractéristiques importantes\n",
    "\n",
    "### Différences avec le ML classique\n",
    "\n",
    "| Aspect | ML Classique | Deep Learning |\n",
    "|--------|--------------|---------------|\n",
    "| **Extraction de features** | Manuelle | Automatique |\n",
    "| **Quantité de données** | Petite à moyenne | Grande |\n",
    "| **Puissance de calcul** | Modérée | Élevée (GPU) |\n",
    "| **Interprétabilité** | Élevée | Faible (boîte noire) |\n",
    "| **Performance** | Bonne | Excellente (données suffisantes) |\n",
    "\n",
    "### Applications en RDC\n",
    "\n",
    "#### 🏥 **Santé**\n",
    "- Diagnostic médical par imagerie (rayons X, échographies)\n",
    "- Détection précoce du paludisme sur images microscopiques\n",
    "- Prédiction d'épidémies (Ebola, COVID-19)\n",
    "\n",
    "#### 🌾 **Agriculture**\n",
    "- Détection de maladies des plantes (manioc, maïs)\n",
    "- Estimation des rendements par images satellites\n",
    "- Classification des sols\n",
    "\n",
    "#### 💰 **Commerce**\n",
    "- Reconnaissance de produits pour e-commerce\n",
    "- Détection de fraudes bancaires\n",
    "- Recommandation de produits\n",
    "\n",
    "#### 📚 **Éducation**\n",
    "- Correction automatique de copies\n",
    "- Chatbots éducatifs en français/lingala\n",
    "- Analyse de performances des étudiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des bibliothèques nécessaires\n",
    "# Exécuter cette cellule une seule fois\n",
    "\n",
    "!pip install tensorflow numpy matplotlib scikit-learn -q\n",
    "\n",
    "print(\"✅ Bibliothèques installées avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports nécessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 2 : Réseaux de neurones artificiels <a id=\"chapitre-2\"></a>\n",
    "\n",
    "## 2.1 Le neurone artificiel\n",
    "\n",
    "Un **neurone artificiel** est l'unité de base d'un réseau de neurones. Il s'inspire du neurone biologique.\n",
    "\n",
    "### Composants d'un neurone\n",
    "\n",
    "1. **Entrées (x₁, x₂, ..., xₙ)** : Les données d'entrée\n",
    "2. **Poids (w₁, w₂, ..., wₙ)** : Importance de chaque entrée\n",
    "3. **Biais (b)** : Valeur ajoutée pour ajuster la sortie\n",
    "4. **Fonction d'activation (f)** : Transforme la somme pondérée\n",
    "\n",
    "### Formule mathématique\n",
    "\n",
    "```\n",
    "z = (x₁ × w₁) + (x₂ × w₂) + ... + (xₙ × wₙ) + b\n",
    "y = f(z)\n",
    "```\n",
    "\n",
    "Où :\n",
    "- **z** = somme pondérée\n",
    "- **y** = sortie du neurone\n",
    "- **f** = fonction d'activation\n",
    "\n",
    "### Fonctions d'activation courantes\n",
    "\n",
    "| Fonction | Formule | Usage |\n",
    "|----------|---------|-------|\n",
    "| **Sigmoid** | σ(z) = 1 / (1 + e⁻ᶻ) | Classification binaire |\n",
    "| **ReLU** | f(z) = max(0, z) | Couches cachées (le plus courant) |\n",
    "| **Tanh** | tanh(z) = (eᶻ - e⁻ᶻ) / (eᶻ + e⁻ᶻ) | Couches cachées |\n",
    "| **Softmax** | σ(z)ᵢ = eᶻⁱ / Σeᶻʲ | Classification multi-classes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des fonctions d'activation\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "# Créer les données\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Créer les graphiques\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(z, sigmoid(z), 'b-', linewidth=2)\n",
    "axes[0].set_title('Sigmoid', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# ReLU\n",
    "axes[1].plot(z, relu(z), 'r-', linewidth=2)\n",
    "axes[1].set_title('ReLU', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Tanh\n",
    "axes[2].plot(z, tanh(z), 'g-', linewidth=2)\n",
    "axes[2].set_title('Tanh', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[2].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Architecture d'un réseau de neurones\n",
    "\n",
    "Un réseau de neurones est composé de plusieurs **couches** :\n",
    "\n",
    "1. **Couche d'entrée** : Reçoit les données\n",
    "2. **Couches cachées** : Effectuent les calculs (peuvent être multiples)\n",
    "3. **Couche de sortie** : Produit la prédiction\n",
    "\n",
    "### Exemple : Classification de fruits\n",
    "\n",
    "```\n",
    "Entrée (4 features)     Couche cachée (8 neurones)     Sortie (3 classes)\n",
    "    [Couleur]  ────────────────────────────────────────  [Banane]\n",
    "    [Forme]    ────────────────────────────────────────  [Mangue]\n",
    "    [Taille]   ────────────────────────────────────────  [Ananas]\n",
    "    [Texture]  ────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple : Perceptron pour classification binaire\n",
    "# Problème : Prédire si un patient a le paludisme (0 = Non, 1 = Oui)\n",
    "\n",
    "# Données synthétiques : [Température, Fatigue, Maux de tête]\n",
    "X = np.array([\n",
    "    [37.0, 2, 1],  # Pas de paludisme\n",
    "    [39.5, 8, 9],  # Paludisme\n",
    "    [38.2, 5, 4],  # Pas de paludisme\n",
    "    [40.1, 9, 10], # Paludisme\n",
    "    [37.5, 3, 2],  # Pas de paludisme\n",
    "    [39.8, 9, 8],  # Paludisme\n",
    "    [36.8, 1, 1],  # Pas de paludisme\n",
    "    [40.5, 10, 9], # Paludisme\n",
    "])\n",
    "\n",
    "y = np.array([0, 1, 0, 1, 0, 1, 0, 1])  # Labels\n",
    "\n",
    "print(\"Données d'entraînement:\")\n",
    "print(\"Features (Température, Fatigue, Maux de tête):\")\n",
    "print(X)\n",
    "print(\"\\nLabels (0=Sain, 1=Paludisme):\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un perceptron simple avec TensorFlow\n",
    "\n",
    "# Normaliser les données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Créer le modèle\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation='sigmoid', input_shape=(3,))\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    X_scaled, y,\n",
    "    epochs=100,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Évaluer\n",
    "loss, accuracy = model.evaluate(X_scaled, y, verbose=0)\n",
    "print(f\"\\n✅ Précision du modèle: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Faire des prédictions\n",
    "predictions = model.predict(X_scaled, verbose=0)\n",
    "print(\"\\nPrédictions:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Patient {i+1}: {pred[0]:.4f} → {'Paludisme' if pred[0] > 0.5 else 'Sain'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercice 1 : Créer votre premier neurone\n",
    "\n",
    "Créez un perceptron pour prédire si un étudiant va réussir son examen.\n",
    "\n",
    "**Données :**\n",
    "- Entrées : [Heures d'étude, Présence en classe (%), Note précédente]\n",
    "- Sortie : 0 = Échec, 1 = Réussite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1 : À vous de jouer !\n",
    "\n",
    "# Données\n",
    "X_etudiants = np.array([\n",
    "    [2, 60, 8],   # Échec\n",
    "    [8, 95, 15],  # Réussite\n",
    "    [5, 75, 12],  # Réussite\n",
    "    [1, 50, 6],   # Échec\n",
    "    [7, 90, 14],  # Réussite\n",
    "    [3, 65, 9],   # Échec\n",
    "])\n",
    "\n",
    "y_etudiants = np.array([0, 1, 1, 0, 1, 0])\n",
    "\n",
    "# TODO: Créez et entraînez votre modèle ici\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 3 : Entraînement d'un réseau <a id=\"chapitre-3\"></a>\n",
    "\n",
    "## 3.1 Propagation avant (Forward Propagation)\n",
    "\n",
    "C'est le processus où les données traversent le réseau de l'entrée vers la sortie.\n",
    "\n",
    "**Étapes :**\n",
    "1. Les données entrent dans la couche d'entrée\n",
    "2. Chaque neurone calcule sa sortie\n",
    "3. Les sorties deviennent les entrées de la couche suivante\n",
    "4. On obtient la prédiction finale\n",
    "\n",
    "## 3.2 Fonction de perte (Loss Function)\n",
    "\n",
    "La fonction de perte mesure l'**erreur** entre la prédiction et la vraie valeur.\n",
    "\n",
    "### Fonctions de perte courantes\n",
    "\n",
    "| Fonction | Usage | Formule |\n",
    "|----------|-------|----------|\n",
    "| **Binary Crossentropy** | Classification binaire | -[y log(ŷ) + (1-y) log(1-ŷ)] |\n",
    "| **Categorical Crossentropy** | Classification multi-classes | -Σ yᵢ log(ŷᵢ) |\n",
    "| **MSE** | Régression | (y - ŷ)² |\n",
    "\n",
    "## 3.3 Backpropagation\n",
    "\n",
    "C'est l'algorithme qui permet d'**ajuster les poids** pour réduire l'erreur.\n",
    "\n",
    "**Analogie simple :**\n",
    "Imaginez que vous apprenez à lancer une balle dans un panier :\n",
    "1. Vous lancez (propagation avant)\n",
    "2. Vous ratez (fonction de perte)\n",
    "3. Vous ajustez votre geste (backpropagation)\n",
    "4. Vous relancez (nouvelle itération)\n",
    "\n",
    "**Processus :**\n",
    "1. Calculer l'erreur de sortie\n",
    "2. Propager l'erreur vers l'arrière\n",
    "3. Calculer les gradients (dérivées)\n",
    "4. Mettre à jour les poids : `w_nouveau = w_ancien - α × gradient`\n",
    "\n",
    "Où **α** (alpha) est le **taux d'apprentissage** (learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Visualiser l'entraînement\n",
    "\n",
    "# Créer des données synthétiques (classification binaire)\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Classe 0 : Produits locaux bon marché\n",
    "X_classe0 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])\n",
    "# Classe 1 : Produits importés chers\n",
    "X_classe1 = np.random.randn(n_samples // 2, 2) + np.array([5, 5])\n",
    "\n",
    "X_train = np.vstack([X_classe0, X_classe1])\n",
    "y_train = np.hstack([np.zeros(n_samples // 2), np.ones(n_samples // 2)])\n",
    "\n",
    "# Mélanger les données\n",
    "indices = np.random.permutation(n_samples)\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# Visualiser les données\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], \n",
    "            c='blue', label='Produits locaux', alpha=0.6, s=50)\n",
    "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], \n",
    "            c='red', label='Produits importés', alpha=0.6, s=50)\n",
    "plt.xlabel('Prix (milliers FC)', fontsize=12)\n",
    "plt.ylabel('Qualité (score)', fontsize=12)\n",
    "plt.title('Classification de produits au marché de Kinshasa', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un réseau avec une couche cachée\n",
    "model_classification = keras.Sequential([\n",
    "    keras.layers.Dense(8, activation='relu', input_shape=(2,)),\n",
    "    keras.layers.Dense(4, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_classification.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Architecture du réseau:\")\n",
    "model_classification.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle et visualiser la progression\n",
    "history = model_classification.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Visualiser l'évolution de la perte et de la précision\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Perte\n",
    "axes[0].plot(history.history['loss'], label='Entraînement', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0].set_xlabel('Époque', fontsize=12)\n",
    "axes[0].set_ylabel('Perte', fontsize=12)\n",
    "axes[0].set_title('Évolution de la perte', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Précision\n",
    "axes[1].plot(history.history['accuracy'], label='Entraînement', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('Époque', fontsize=12)\n",
    "axes[1].set_ylabel('Précision', fontsize=12)\n",
    "axes[1].set_title('Évolution de la précision', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Résultats finaux\n",
    "final_loss = history.history['val_loss'][-1]\n",
    "final_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"\\n✅ Résultats finaux:\")\n",
    "print(f\"   Perte de validation: {final_loss:.4f}\")\n",
    "print(f\"   Précision de validation: {final_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercice 2 : Comprendre l'impact du learning rate\n",
    "\n",
    "Testez différents taux d'apprentissage et observez l'impact sur l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2 : Tester différents learning rates\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "# TODO: Entraînez 4 modèles avec ces différents learning rates\n",
    "# TODO: Comparez les résultats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 4 : TensorFlow et PyTorch <a id=\"chapitre-4\"></a>\n",
    "\n",
    "## 4.1 Introduction à TensorFlow\n",
    "\n",
    "**TensorFlow** est une bibliothèque open-source développée par Google pour le Deep Learning.\n",
    "\n",
    "### Avantages\n",
    "- ✅ Très populaire et bien documenté\n",
    "- ✅ API Keras intégrée (simple à utiliser)\n",
    "- ✅ Déploiement facile en production\n",
    "- ✅ Support mobile (TensorFlow Lite)\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple complet avec TensorFlow : Prédire le prix du manioc\n",
    "\n",
    "# Données synthétiques : [Saison (0-3), Pluie (mm), Température (°C)]\n",
    "X_manioc = np.array([\n",
    "    [0, 50, 25],   # Saison sèche, peu de pluie\n",
    "    [1, 150, 28],  # Saison des pluies\n",
    "    [2, 200, 26],  # Pic des pluies\n",
    "    [3, 80, 27],   # Fin des pluies\n",
    "    [0, 45, 26],\n",
    "    [1, 160, 29],\n",
    "    [2, 190, 25],\n",
    "    [3, 75, 28],\n",
    "    [0, 55, 24],\n",
    "    [1, 145, 27],\n",
    "])\n",
    "\n",
    "# Prix du manioc (en milliers de FC par sac)\n",
    "y_prix = np.array([45, 35, 30, 38, 46, 34, 31, 39, 44, 36])\n",
    "\n",
    "# Normaliser\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_manioc_scaled = scaler_X.fit_transform(X_manioc)\n",
    "y_prix_scaled = scaler_y.fit_transform(y_prix.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Créer le modèle\n",
    "model_manioc = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation='relu', input_shape=(3,)),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(1)  # Pas d'activation pour la régression\n",
    "])\n",
    "\n",
    "model_manioc.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Entraîner\n",
    "history_manioc = model_manioc.fit(\n",
    "    X_manioc_scaled, y_prix_scaled,\n",
    "    epochs=200,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Prédire\n",
    "predictions_scaled = model_manioc.predict(X_manioc_scaled, verbose=0)\n",
    "predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Prédictions de prix du manioc:\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(y_prix)):\n",
    "    print(f\"Réel: {y_prix[i]:.0f} FC | Prédit: {predictions[i][0]:.0f} FC | Erreur: {abs(y_prix[i] - predictions[i][0]):.0f} FC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Introduction à PyTorch\n",
    "\n",
    "**PyTorch** est une bibliothèque développée par Facebook (Meta) pour le Deep Learning.\n",
    "\n",
    "### Avantages\n",
    "- ✅ Plus pythonique et intuitif\n",
    "- ✅ Excellent pour la recherche\n",
    "- ✅ Débogage plus facile\n",
    "- ✅ Graphes dynamiques\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install torch torchvision\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de PyTorch\n",
    "!pip install torch torchvision -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Même exemple avec PyTorch\n",
    "\n",
    "# Définir le modèle\n",
    "class ManiocPriceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ManiocPriceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Créer le modèle\n",
    "model_pytorch = ManiocPriceModel()\n",
    "\n",
    "# Définir la fonction de perte et l'optimiseur\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_pytorch.parameters(), lr=0.01)\n",
    "\n",
    "# Convertir les données en tenseurs PyTorch\n",
    "X_tensor = torch.FloatTensor(X_manioc_scaled)\n",
    "y_tensor = torch.FloatTensor(y_prix_scaled).reshape(-1, 1)\n",
    "\n",
    "# Entraîner\n",
    "losses = []\n",
    "for epoch in range(200):\n",
    "    # Forward pass\n",
    "    predictions = model_pytorch(X_tensor)\n",
    "    loss = criterion(predictions, y_tensor)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "\n",
    "# Visualiser l'entraînement\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Époque', fontsize=12)\n",
    "plt.ylabel('Perte (MSE)', fontsize=12)\n",
    "plt.title('Entraînement avec PyTorch', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Perte finale: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison TensorFlow vs PyTorch\n",
    "\n",
    "| Aspect | TensorFlow | PyTorch |\n",
    "|--------|------------|----------|\n",
    "| **Facilité d'apprentissage** | ⭐⭐⭐⭐ (avec Keras) | ⭐⭐⭐⭐⭐ |\n",
    "| **Production** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ |\n",
    "| **Recherche** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| **Débogage** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| **Communauté** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "\n",
    "**Recommandation pour débutants :** TensorFlow avec Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercice 3 : Créer votre premier réseau\n",
    "\n",
    "Créez un réseau de neurones pour prédire le rendement agricole.\n",
    "\n",
    "**Données :**\n",
    "- Entrées : [Pluie (mm), Engrais (kg), Température (°C)]\n",
    "- Sortie : Rendement (tonnes/hectare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3 : Prédiction de rendement agricole\n",
    "\n",
    "X_agriculture = np.array([\n",
    "    [800, 50, 25],\n",
    "    [1200, 80, 27],\n",
    "    [600, 30, 24],\n",
    "    [1000, 60, 26],\n",
    "    [1400, 90, 28],\n",
    "    [700, 40, 25],\n",
    "])\n",
    "\n",
    "y_rendement = np.array([3.5, 5.2, 2.8, 4.1, 5.8, 3.2])\n",
    "\n",
    "# TODO: Créez et entraînez votre modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 5 : Problèmes et solutions <a id=\"chapitre-5\"></a>\n",
    "\n",
    "## 5.1 Surapprentissage (Overfitting)\n",
    "\n",
    "Le **surapprentissage** se produit quand le modèle apprend \"par cœur\" les données d'entraînement mais ne généralise pas bien.\n",
    "\n",
    "### Signes de surapprentissage\n",
    "- ✅ Excellente performance sur les données d'entraînement\n",
    "- ❌ Mauvaise performance sur les données de test\n",
    "- 📈 Écart croissant entre perte d'entraînement et de validation\n",
    "\n",
    "### Causes\n",
    "1. Modèle trop complexe (trop de paramètres)\n",
    "2. Pas assez de données d'entraînement\n",
    "3. Entraînement trop long\n",
    "\n",
    "## 5.2 Sous-apprentissage (Underfitting)\n",
    "\n",
    "Le **sous-apprentissage** se produit quand le modèle est trop simple et ne capture pas les patterns.\n",
    "\n",
    "### Signes de sous-apprentissage\n",
    "- ❌ Mauvaise performance sur l'entraînement\n",
    "- ❌ Mauvaise performance sur le test\n",
    "- 📉 Perte élevée qui ne diminue pas\n",
    "\n",
    "## 5.3 Solutions\n",
    "\n",
    "### 1. Validation croisée (Cross-validation)\n",
    "\n",
    "Diviser les données en 3 ensembles :\n",
    "- **Entraînement (70%)** : Pour apprendre\n",
    "- **Validation (15%)** : Pour ajuster les hyperparamètres\n",
    "- **Test (15%)** : Pour évaluer la performance finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Split des données\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Créer des données synthétiques\n",
    "X_data = np.random.randn(1000, 5)\n",
    "y_data = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# Split 1 : Séparer test (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Split 2 : Séparer validation (15% du reste)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42  # 0.176 * 0.85 ≈ 0.15\n",
    ")\n",
    "\n",
    "print(f\"Taille de l'ensemble d'entraînement: {len(X_train)} ({len(X_train)/len(X_data)*100:.1f}%)\")\n",
    "print(f\"Taille de l'ensemble de validation: {len(X_val)} ({len(X_val)/len(X_data)*100:.1f}%)\")\n",
    "print(f\"Taille de l'ensemble de test: {len(X_test)} ({len(X_test)/len(X_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dropout\n",
    "\n",
    "Le **dropout** désactive aléatoirement des neurones pendant l'entraînement pour éviter le surapprentissage.\n",
    "\n",
    "**Analogie :** C'est comme étudier avec différents groupes d'amis. Vous ne dépendez pas toujours des mêmes personnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle avec Dropout\n",
    "\n",
    "model_dropout = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(5,)),\n",
    "    keras.layers.Dropout(0.3),  # Désactive 30% des neurones\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_dropout.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Régularisation L1/L2\n",
    "\n",
    "La **régularisation** pénalise les poids trop grands pour simplifier le modèle.\n",
    "\n",
    "- **L1** : Pousse certains poids vers 0 (sélection de features)\n",
    "- **L2** : Réduit tous les poids (plus courant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle avec régularisation L2\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model_regularized = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', \n",
    "                      kernel_regularizer=regularizers.l2(0.01),\n",
    "                      input_shape=(5,)),\n",
    "    keras.layers.Dense(32, activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_regularized.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Modèle avec régularisation L2:\")\n",
    "model_regularized.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Early Stopping\n",
    "\n",
    "Arrêter l'entraînement quand la performance sur la validation ne s'améliore plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Attendre 10 époques sans amélioration\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Entraîner avec early stopping\n",
    "history_early = model_dropout.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"✅ Entraînement arrêté à l'époque {len(history_early.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercice 4 : Combattre le surapprentissage\n",
    "\n",
    "Créez deux modèles et comparez-les :\n",
    "1. Un modèle simple sans régularisation\n",
    "2. Un modèle avec Dropout et Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 4 : Comparer les modèles\n",
    "\n",
    "# TODO: Créez deux modèles et comparez leurs performances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 6 : Projet final - Classification de fruits <a id=\"chapitre-6\"></a>\n",
    "\n",
    "## 🎯 Objectif du projet\n",
    "\n",
    "Créer un réseau de neurones pour classifier des fruits locaux congolais :\n",
    "- 🍌 **Banane**\n",
    "- 🥭 **Mangue**\n",
    "- 🍍 **Ananas**\n",
    "\n",
    "## Étapes du projet\n",
    "\n",
    "1. Créer un dataset synthétique de features\n",
    "2. Construire un réseau de neurones\n",
    "3. Entraîner le modèle\n",
    "4. Évaluer les performances\n",
    "5. Faire des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Créer le dataset\n",
    "\n",
    "# Features : [Longueur (cm), Largeur (cm), Poids (g), Couleur (0-1), Texture (0-1)]\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Bananes : allongées, jaunes\n",
    "bananes = np.random.randn(100, 5) * np.array([2, 1, 20, 0.1, 0.1]) + np.array([18, 4, 120, 0.8, 0.3])\n",
    "\n",
    "# Mangues : ovales, orange-rouge\n",
    "mangues = np.random.randn(100, 5) * np.array([2, 2, 30, 0.1, 0.1]) + np.array([12, 8, 250, 0.6, 0.5])\n",
    "\n",
    "# Ananas : gros, rugueux\n",
    "ananas = np.random.randn(100, 5) * np.array([3, 3, 50, 0.1, 0.1]) + np.array([25, 12, 1000, 0.4, 0.8])\n",
    "\n",
    "# Combiner les données\n",
    "X_fruits = np.vstack([bananes, mangues, ananas])\n",
    "y_fruits = np.hstack([\n",
    "    np.zeros(100),  # Bananes = 0\n",
    "    np.ones(100),   # Mangues = 1\n",
    "    np.full(100, 2) # Ananas = 2\n",
    "])\n",
    "\n",
    "# Mélanger\n",
    "indices = np.random.permutation(300)\n",
    "X_fruits = X_fruits[indices]\n",
    "y_fruits = y_fruits[indices]\n",
    "\n",
    "print(\"Dataset créé:\")\n",
    "print(f\"- Nombre d'exemples: {len(X_fruits)}\")\n",
    "print(f\"- Features par exemple: {X_fruits.shape[1]}\")\n",
    "print(f\"- Classes: 3 (Banane, Mangue, Ananas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les données\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Graphique 1 : Longueur vs Largeur\n",
    "ax1 = fig.add_subplot(121)\n",
    "colors = ['yellow', 'orange', 'green']\n",
    "labels = ['Banane', 'Mangue', 'Ananas']\n",
    "\n",
    "for i in range(3):\n",
    "    mask = y_fruits == i\n",
    "    ax1.scatter(X_fruits[mask, 0], X_fruits[mask, 1], \n",
    "               c=colors[i], label=labels[i], alpha=0.6, s=50)\n",
    "\n",
    "ax1.set_xlabel('Longueur (cm)', fontsize=12)\n",
    "ax1.set_ylabel('Largeur (cm)', fontsize=12)\n",
    "ax1.set_title('Dimensions des fruits', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 2 : Poids vs Texture\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "for i in range(3):\n",
    "    mask = y_fruits == i\n",
    "    ax2.scatter(X_fruits[mask, 2], X_fruits[mask, 4], \n",
    "               c=colors[i], label=labels[i], alpha=0.6, s=50)\n",
    "\n",
    "ax2.set_xlabel('Poids (g)', fontsize=12)\n",
    "ax2.set_ylabel('Texture', fontsize=12)\n",
    "ax2.set_title('Poids et texture des fruits', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 2 : Préparer les données\n",
    "\n",
    "# Split train/test\n",
    "X_train_fruits, X_test_fruits, y_train_fruits, y_test_fruits = train_test_split(\n",
    "    X_fruits, y_fruits, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normaliser\n",
    "scaler_fruits = StandardScaler()\n",
    "X_train_fruits_scaled = scaler_fruits.fit_transform(X_train_fruits)\n",
    "X_test_fruits_scaled = scaler_fruits.transform(X_test_fruits)\n",
    "\n",
    "# Convertir les labels en one-hot encoding\n",
    "y_train_fruits_cat = keras.utils.to_categorical(y_train_fruits, 3)\n",
    "y_test_fruits_cat = keras.utils.to_categorical(y_test_fruits, 3)\n",
    "\n",
    "print(\"Données préparées:\")\n",
    "print(f\"- Entraînement: {len(X_train_fruits)} exemples\")\n",
    "print(f\"- Test: {len(X_test_fruits)} exemples\")\n",
    "print(f\"- Shape des labels: {y_train_fruits_cat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 3 : Construire le modèle\n",
    "\n",
    "model_fruits = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(5,)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')  # 3 classes\n",
    "])\n",
    "\n",
    "model_fruits.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Architecture du modèle:\")\n",
    "model_fruits.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 4 : Entraîner le modèle\n",
    "\n",
    "early_stop_fruits = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history_fruits = model_fruits.fit(\n",
    "    X_train_fruits_scaled, y_train_fruits_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop_fruits],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Visualiser l'entraînement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Perte\n",
    "axes[0].plot(history_fruits.history['loss'], label='Entraînement', linewidth=2)\n",
    "axes[0].plot(history_fruits.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0].set_xlabel('Époque', fontsize=12)\n",
    "axes[0].set_ylabel('Perte', fontsize=12)\n",
    "axes[0].set_title('Évolution de la perte', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Précision\n",
    "axes[1].plot(history_fruits.history['accuracy'], label='Entraînement', linewidth=2)\n",
    "axes[1].plot(history_fruits.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('Époque', fontsize=12)\n",
    "axes[1].set_ylabel('Précision', fontsize=12)\n",
    "axes[1].set_title('Évolution de la précision', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Entraînement terminé en {len(history_fruits.history['loss'])} époques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 5 : Évaluer le modèle\n",
    "\n",
    "test_loss, test_accuracy = model_fruits.evaluate(\n",
    "    X_test_fruits_scaled, y_test_fruits_cat, verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📊 RÉSULTATS FINAUX\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Perte sur le test: {test_loss:.4f}\")\n",
    "print(f\"Précision sur le test: {test_accuracy * 100:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Prédictions\n",
    "y_pred_proba = model_fruits.predict(X_test_fruits_scaled, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test_fruits, y_pred)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Banane', 'Mangue', 'Ananas'],\n",
    "            yticklabels=['Banane', 'Mangue', 'Ananas'])\n",
    "plt.xlabel('Prédiction', fontsize=12)\n",
    "plt.ylabel('Vérité', fontsize=12)\n",
    "plt.title('Matrice de confusion - Classification de fruits', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(y_test_fruits, y_pred, \n",
    "                          target_names=['Banane', 'Mangue', 'Ananas']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 6 : Faire des prédictions sur de nouveaux fruits\n",
    "\n",
    "# Nouveaux fruits à classifier\n",
    "nouveaux_fruits = np.array([\n",
    "    [20, 4, 130, 0.85, 0.25],  # Devrait être une banane\n",
    "    [11, 7, 240, 0.55, 0.48],  # Devrait être une mangue\n",
    "    [26, 13, 1050, 0.38, 0.82], # Devrait être un ananas\n",
    "])\n",
    "\n",
    "# Normaliser\n",
    "nouveaux_fruits_scaled = scaler_fruits.transform(nouveaux_fruits)\n",
    "\n",
    "# Prédire\n",
    "predictions = model_fruits.predict(nouveaux_fruits_scaled, verbose=0)\n",
    "\n",
    "# Afficher les résultats\n",
    "fruit_names = ['Banane', 'Mangue', 'Ananas']\n",
    "\n",
    "print(\"\\n🍎 PRÉDICTIONS SUR DE NOUVEAUX FRUITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    predicted_class = np.argmax(pred)\n",
    "    confidence = pred[predicted_class] * 100\n",
    "    \n",
    "    print(f\"\\nFruit {i+1}:\")\n",
    "    print(f\"  Caractéristiques: {nouveaux_fruits[i]}\")\n",
    "    print(f\"  Prédiction: {fruit_names[predicted_class]}\")\n",
    "    print(f\"  Confiance: {confidence:.2f}%\")\n",
    "    print(f\"  Probabilités:\")\n",
    "    for j, prob in enumerate(pred):\n",
    "        print(f\"    - {fruit_names[j]}: {prob*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercice 5 : Améliorer le modèle\n",
    "\n",
    "Essayez d'améliorer la performance du modèle en :\n",
    "1. Ajoutant plus de couches\n",
    "2. Modifiant le nombre de neurones\n",
    "3. Testant différentes fonctions d'activation\n",
    "4. Ajustant le dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5 : Améliorez le modèle\n",
    "\n",
    "# TODO: Créez une version améliorée du modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎓 Résumé du module\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **Deep Learning vs ML classique**\n",
    "   - Extraction automatique de features\n",
    "   - Besoin de grandes quantités de données\n",
    "   - Applications en RDC\n",
    "\n",
    "2. **Réseaux de neurones**\n",
    "   - Architecture (couches, neurones)\n",
    "   - Fonctions d'activation (ReLU, Sigmoid, Softmax)\n",
    "   - Propagation avant et arrière\n",
    "\n",
    "3. **Entraînement**\n",
    "   - Fonctions de perte\n",
    "   - Backpropagation\n",
    "   - Optimiseurs (Adam, SGD)\n",
    "\n",
    "4. **Frameworks**\n",
    "   - TensorFlow/Keras\n",
    "   - PyTorch\n",
    "   - Comparaison et choix\n",
    "\n",
    "5. **Problèmes courants**\n",
    "   - Surapprentissage et solutions\n",
    "   - Dropout, régularisation\n",
    "   - Early stopping\n",
    "\n",
    "6. **Projet pratique**\n",
    "   - Classification multi-classes\n",
    "   - Évaluation de performance\n",
    "   - Matrice de confusion\n",
    "\n",
    "### Prochaines étapes\n",
    "\n",
    "1. **Réseaux convolutifs (CNN)**\n",
    "   - Traitement d'images\n",
    "   - Détection d'objets\n",
    "\n",
    "2. **Réseaux récurrents (RNN/LSTM)**\n",
    "   - Séries temporelles\n",
    "   - Traitement du langage\n",
    "\n",
    "3. **Transfer Learning**\n",
    "   - Utiliser des modèles pré-entraînés\n",
    "   - Fine-tuning\n",
    "\n",
    "4. **Déploiement**\n",
    "   - API REST\n",
    "   - Applications mobiles\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Ressources supplémentaires\n",
    "\n",
    "### Cours en ligne\n",
    "- [Deep Learning Specialization - Coursera](https://www.coursera.org/specializations/deep-learning)\n",
    "- [Fast.ai - Practical Deep Learning](https://www.fast.ai/)\n",
    "- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)\n",
    "\n",
    "### Livres\n",
    "- \"Deep Learning\" - Ian Goodfellow\n",
    "- \"Hands-On Machine Learning\" - Aurélien Géron\n",
    "- \"Neural Networks and Deep Learning\" - Michael Nielsen (gratuit)\n",
    "\n",
    "### Communautés\n",
    "- [Kaggle](https://www.kaggle.com/) - Compétitions et datasets\n",
    "- [Papers With Code](https://paperswithcode.com/) - Dernières recherches\n",
    "- [Reddit r/MachineLearning](https://www.reddit.com/r/MachineLearning/)\n",
    "\n",
    "---\n",
    "\n",
    "**Félicitations ! Vous avez terminé le module Deep Learning ! 🎉**\n",
    "\n",
    "*Continuez à pratiquer et à explorer. Le Deep Learning est un domaine en constante évolution !*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
