{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Deep Learning - Formation Compl√®te\n",
    "\n",
    "## Plateforme IA-Solution RDC\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objectifs du module\n",
    "\n",
    "√Ä la fin de ce module, vous serez capable de :\n",
    "- Comprendre ce qu'est le Deep Learning et ses diff√©rences avec le ML classique\n",
    "- Ma√Ætriser les r√©seaux de neurones artificiels (architecture, fonctionnement)\n",
    "- Impl√©menter des r√©seaux avec TensorFlow et PyTorch\n",
    "- R√©soudre des probl√®mes de classification et r√©gression\n",
    "- Appliquer le Deep Learning √† des cas concrets en RDC\n",
    "\n",
    "**Niveau :** Interm√©diaire  \n",
    "**Pr√©requis :** Python, bases de Machine Learning  \n",
    "**Dur√©e :** 10 semaines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Table des mati√®res\n",
    "\n",
    "1. [Introduction au Deep Learning](#chapitre-1)\n",
    "2. [R√©seaux de neurones artificiels](#chapitre-2)\n",
    "3. [Entra√Ænement d'un r√©seau](#chapitre-3)\n",
    "4. [TensorFlow et PyTorch](#chapitre-4)\n",
    "5. [Probl√®mes et solutions](#chapitre-5)\n",
    "6. [Projet final : Classification de fruits](#chapitre-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 1 : Introduction au Deep Learning <a id=\"chapitre-1\"></a>\n",
    "\n",
    "## 1.1 Qu'est-ce que le Deep Learning ?\n",
    "\n",
    "Le **Deep Learning** (apprentissage profond) est une branche du Machine Learning qui utilise des **r√©seaux de neurones artificiels** avec plusieurs couches pour apprendre des repr√©sentations complexes des donn√©es.\n",
    "\n",
    "### Analogie simple\n",
    "\n",
    "Imaginez que vous apprenez √† un enfant √† reconna√Ætre des fruits :\n",
    "- **Machine Learning classique** : Vous lui donnez des r√®gles pr√©cises (\"si c'est jaune et allong√©, c'est une banane\")\n",
    "- **Deep Learning** : Vous lui montrez des milliers d'images de fruits, et il apprend lui-m√™me les caract√©ristiques importantes\n",
    "\n",
    "### Diff√©rences avec le ML classique\n",
    "\n",
    "| Aspect | ML Classique | Deep Learning |\n",
    "|--------|--------------|---------------|\n",
    "| **Extraction de features** | Manuelle | Automatique |\n",
    "| **Quantit√© de donn√©es** | Petite √† moyenne | Grande |\n",
    "| **Puissance de calcul** | Mod√©r√©e | √âlev√©e (GPU) |\n",
    "| **Interpr√©tabilit√©** | √âlev√©e | Faible (bo√Æte noire) |\n",
    "| **Performance** | Bonne | Excellente (donn√©es suffisantes) |\n",
    "\n",
    "### Applications en RDC\n",
    "\n",
    "#### üè• **Sant√©**\n",
    "- Diagnostic m√©dical par imagerie (rayons X, √©chographies)\n",
    "- D√©tection pr√©coce du paludisme sur images microscopiques\n",
    "- Pr√©diction d'√©pid√©mies (Ebola, COVID-19)\n",
    "\n",
    "#### üåæ **Agriculture**\n",
    "- D√©tection de maladies des plantes (manioc, ma√Øs)\n",
    "- Estimation des rendements par images satellites\n",
    "- Classification des sols\n",
    "\n",
    "#### üí∞ **Commerce**\n",
    "- Reconnaissance de produits pour e-commerce\n",
    "- D√©tection de fraudes bancaires\n",
    "- Recommandation de produits\n",
    "\n",
    "#### üìö **√âducation**\n",
    "- Correction automatique de copies\n",
    "- Chatbots √©ducatifs en fran√ßais/lingala\n",
    "- Analyse de performances des √©tudiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des biblioth√®ques n√©cessaires\n",
    "# Ex√©cuter cette cellule une seule fois\n",
    "\n",
    "!pip install tensorflow numpy matplotlib scikit-learn -q\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques install√©es avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports n√©cessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 2 : R√©seaux de neurones artificiels <a id=\"chapitre-2\"></a>\n",
    "\n",
    "## 2.1 Le neurone artificiel\n",
    "\n",
    "Un **neurone artificiel** est l'unit√© de base d'un r√©seau de neurones. Il s'inspire du neurone biologique.\n",
    "\n",
    "### Composants d'un neurone\n",
    "\n",
    "1. **Entr√©es (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô)** : Les donn√©es d'entr√©e\n",
    "2. **Poids (w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)** : Importance de chaque entr√©e\n",
    "3. **Biais (b)** : Valeur ajout√©e pour ajuster la sortie\n",
    "4. **Fonction d'activation (f)** : Transforme la somme pond√©r√©e\n",
    "\n",
    "### Formule math√©matique\n",
    "\n",
    "```\n",
    "z = (x‚ÇÅ √ó w‚ÇÅ) + (x‚ÇÇ √ó w‚ÇÇ) + ... + (x‚Çô √ó w‚Çô) + b\n",
    "y = f(z)\n",
    "```\n",
    "\n",
    "O√π :\n",
    "- **z** = somme pond√©r√©e\n",
    "- **y** = sortie du neurone\n",
    "- **f** = fonction d'activation\n",
    "\n",
    "### Fonctions d'activation courantes\n",
    "\n",
    "| Fonction | Formule | Usage |\n",
    "|----------|---------|-------|\n",
    "| **Sigmoid** | œÉ(z) = 1 / (1 + e‚Åª·∂ª) | Classification binaire |\n",
    "| **ReLU** | f(z) = max(0, z) | Couches cach√©es (le plus courant) |\n",
    "| **Tanh** | tanh(z) = (e·∂ª - e‚Åª·∂ª) / (e·∂ª + e‚Åª·∂ª) | Couches cach√©es |\n",
    "| **Softmax** | œÉ(z)·µ¢ = e·∂ª‚Å± / Œ£e·∂ª ≤ | Classification multi-classes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des fonctions d'activation\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "# Cr√©er les donn√©es\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Cr√©er les graphiques\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(z, sigmoid(z), 'b-', linewidth=2)\n",
    "axes[0].set_title('Sigmoid', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# ReLU\n",
    "axes[1].plot(z, relu(z), 'r-', linewidth=2)\n",
    "axes[1].set_title('ReLU', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Tanh\n",
    "axes[2].plot(z, tanh(z), 'g-', linewidth=2)\n",
    "axes[2].set_title('Tanh', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[2].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Architecture d'un r√©seau de neurones\n",
    "\n",
    "Un r√©seau de neurones est compos√© de plusieurs **couches** :\n",
    "\n",
    "1. **Couche d'entr√©e** : Re√ßoit les donn√©es\n",
    "2. **Couches cach√©es** : Effectuent les calculs (peuvent √™tre multiples)\n",
    "3. **Couche de sortie** : Produit la pr√©diction\n",
    "\n",
    "### Exemple : Classification de fruits\n",
    "\n",
    "```\n",
    "Entr√©e (4 features)     Couche cach√©e (8 neurones)     Sortie (3 classes)\n",
    "    [Couleur]  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  [Banane]\n",
    "    [Forme]    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  [Mangue]\n",
    "    [Taille]   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  [Ananas]\n",
    "    [Texture]  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple : Perceptron pour classification binaire\n",
    "# Probl√®me : Pr√©dire si un patient a le paludisme (0 = Non, 1 = Oui)\n",
    "\n",
    "# Donn√©es synth√©tiques : [Temp√©rature, Fatigue, Maux de t√™te]\n",
    "X = np.array([\n",
    "    [37.0, 2, 1],  # Pas de paludisme\n",
    "    [39.5, 8, 9],  # Paludisme\n",
    "    [38.2, 5, 4],  # Pas de paludisme\n",
    "    [40.1, 9, 10], # Paludisme\n",
    "    [37.5, 3, 2],  # Pas de paludisme\n",
    "    [39.8, 9, 8],  # Paludisme\n",
    "    [36.8, 1, 1],  # Pas de paludisme\n",
    "    [40.5, 10, 9], # Paludisme\n",
    "])\n",
    "\n",
    "y = np.array([0, 1, 0, 1, 0, 1, 0, 1])  # Labels\n",
    "\n",
    "print(\"Donn√©es d'entra√Ænement:\")\n",
    "print(\"Features (Temp√©rature, Fatigue, Maux de t√™te):\")\n",
    "print(X)\n",
    "print(\"\\nLabels (0=Sain, 1=Paludisme):\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un perceptron simple avec TensorFlow\n",
    "\n",
    "# Normaliser les donn√©es\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation='sigmoid', input_shape=(3,))\n",
    "])\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner le mod√®le\n",
    "history = model.fit(\n",
    "    X_scaled, y,\n",
    "    epochs=100,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# √âvaluer\n",
    "loss, accuracy = model.evaluate(X_scaled, y, verbose=0)\n",
    "print(f\"\\n‚úÖ Pr√©cision du mod√®le: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Faire des pr√©dictions\n",
    "predictions = model.predict(X_scaled, verbose=0)\n",
    "print(\"\\nPr√©dictions:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Patient {i+1}: {pred[0]:.4f} ‚Üí {'Paludisme' if pred[0] > 0.5 else 'Sain'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercice 1 : Cr√©er votre premier neurone\n",
    "\n",
    "Cr√©ez un perceptron pour pr√©dire si un √©tudiant va r√©ussir son examen.\n",
    "\n",
    "**Donn√©es :**\n",
    "- Entr√©es : [Heures d'√©tude, Pr√©sence en classe (%), Note pr√©c√©dente]\n",
    "- Sortie : 0 = √âchec, 1 = R√©ussite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1 : √Ä vous de jouer !\n",
    "\n",
    "# Donn√©es\n",
    "X_etudiants = np.array([\n",
    "    [2, 60, 8],   # √âchec\n",
    "    [8, 95, 15],  # R√©ussite\n",
    "    [5, 75, 12],  # R√©ussite\n",
    "    [1, 50, 6],   # √âchec\n",
    "    [7, 90, 14],  # R√©ussite\n",
    "    [3, 65, 9],   # √âchec\n",
    "])\n",
    "\n",
    "y_etudiants = np.array([0, 1, 1, 0, 1, 0])\n",
    "\n",
    "# TODO: Cr√©ez et entra√Ænez votre mod√®le ici\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 3 : Entra√Ænement d'un r√©seau <a id=\"chapitre-3\"></a>\n",
    "\n",
    "## 3.1 Propagation avant (Forward Propagation)\n",
    "\n",
    "C'est le processus o√π les donn√©es traversent le r√©seau de l'entr√©e vers la sortie.\n",
    "\n",
    "**√âtapes :**\n",
    "1. Les donn√©es entrent dans la couche d'entr√©e\n",
    "2. Chaque neurone calcule sa sortie\n",
    "3. Les sorties deviennent les entr√©es de la couche suivante\n",
    "4. On obtient la pr√©diction finale\n",
    "\n",
    "## 3.2 Fonction de perte (Loss Function)\n",
    "\n",
    "La fonction de perte mesure l'**erreur** entre la pr√©diction et la vraie valeur.\n",
    "\n",
    "### Fonctions de perte courantes\n",
    "\n",
    "| Fonction | Usage | Formule |\n",
    "|----------|-------|----------|\n",
    "| **Binary Crossentropy** | Classification binaire | -[y log(≈∑) + (1-y) log(1-≈∑)] |\n",
    "| **Categorical Crossentropy** | Classification multi-classes | -Œ£ y·µ¢ log(≈∑·µ¢) |\n",
    "| **MSE** | R√©gression | (y - ≈∑)¬≤ |\n",
    "\n",
    "## 3.3 Backpropagation\n",
    "\n",
    "C'est l'algorithme qui permet d'**ajuster les poids** pour r√©duire l'erreur.\n",
    "\n",
    "**Analogie simple :**\n",
    "Imaginez que vous apprenez √† lancer une balle dans un panier :\n",
    "1. Vous lancez (propagation avant)\n",
    "2. Vous ratez (fonction de perte)\n",
    "3. Vous ajustez votre geste (backpropagation)\n",
    "4. Vous relancez (nouvelle it√©ration)\n",
    "\n",
    "**Processus :**\n",
    "1. Calculer l'erreur de sortie\n",
    "2. Propager l'erreur vers l'arri√®re\n",
    "3. Calculer les gradients (d√©riv√©es)\n",
    "4. Mettre √† jour les poids : `w_nouveau = w_ancien - Œ± √ó gradient`\n",
    "\n",
    "O√π **Œ±** (alpha) est le **taux d'apprentissage** (learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Visualiser l'entra√Ænement\n",
    "\n",
    "# Cr√©er des donn√©es synth√©tiques (classification binaire)\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Classe 0 : Produits locaux bon march√©\n",
    "X_classe0 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])\n",
    "# Classe 1 : Produits import√©s chers\n",
    "X_classe1 = np.random.randn(n_samples // 2, 2) + np.array([5, 5])\n",
    "\n",
    "X_train = np.vstack([X_classe0, X_classe1])\n",
    "y_train = np.hstack([np.zeros(n_samples // 2), np.ones(n_samples // 2)])\n",
    "\n",
    "# M√©langer les donn√©es\n",
    "indices = np.random.permutation(n_samples)\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# Visualiser les donn√©es\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], \n",
    "            c='blue', label='Produits locaux', alpha=0.6, s=50)\n",
    "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], \n",
    "            c='red', label='Produits import√©s', alpha=0.6, s=50)\n",
    "plt.xlabel('Prix (milliers FC)', fontsize=12)\n",
    "plt.ylabel('Qualit√© (score)', fontsize=12)\n",
    "plt.title('Classification de produits au march√© de Kinshasa', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un r√©seau avec une couche cach√©e\n",
    "model_classification = keras.Sequential([\n",
    "    keras.layers.Dense(8, activation='relu', input_shape=(2,)),\n",
    "    keras.layers.Dense(4, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_classification.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Architecture du r√©seau:\")\n",
    "model_classification.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner le mod√®le et visualiser la progression\n",
    "history = model_classification.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Visualiser l'√©volution de la perte et de la pr√©cision\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Perte\n",
    "axes[0].plot(history.history['loss'], label='Entra√Ænement', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0].set_xlabel('√âpoque', fontsize=12)\n",
    "axes[0].set_ylabel('Perte', fontsize=12)\n",
    "axes[0].set_title('√âvolution de la perte', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Pr√©cision\n",
    "axes[1].plot(history.history['accuracy'], label='Entra√Ænement', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('√âpoque', fontsize=12)\n",
    "axes[1].set_ylabel('Pr√©cision', fontsize=12)\n",
    "axes[1].set_title('√âvolution de la pr√©cision', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# R√©sultats finaux\n",
    "final_loss = history.history['val_loss'][-1]\n",
    "final_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"\\n‚úÖ R√©sultats finaux:\")\n",
    "print(f\"   Perte de validation: {final_loss:.4f}\")\n",
    "print(f\"   Pr√©cision de validation: {final_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercice 2 : Comprendre l'impact du learning rate\n",
    "\n",
    "Testez diff√©rents taux d'apprentissage et observez l'impact sur l'entra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2 : Tester diff√©rents learning rates\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "# TODO: Entra√Ænez 4 mod√®les avec ces diff√©rents learning rates\n",
    "# TODO: Comparez les r√©sultats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 4 : TensorFlow et PyTorch <a id=\"chapitre-4\"></a>\n",
    "\n",
    "## 4.1 Introduction √† TensorFlow\n",
    "\n",
    "**TensorFlow** est une biblioth√®que open-source d√©velopp√©e par Google pour le Deep Learning.\n",
    "\n",
    "### Avantages\n",
    "- ‚úÖ Tr√®s populaire et bien document√©\n",
    "- ‚úÖ API Keras int√©gr√©e (simple √† utiliser)\n",
    "- ‚úÖ D√©ploiement facile en production\n",
    "- ‚úÖ Support mobile (TensorFlow Lite)\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple complet avec TensorFlow : Pr√©dire le prix du manioc\n",
    "\n",
    "# Donn√©es synth√©tiques : [Saison (0-3), Pluie (mm), Temp√©rature (¬∞C)]\n",
    "X_manioc = np.array([\n",
    "    [0, 50, 25],   # Saison s√®che, peu de pluie\n",
    "    [1, 150, 28],  # Saison des pluies\n",
    "    [2, 200, 26],  # Pic des pluies\n",
    "    [3, 80, 27],   # Fin des pluies\n",
    "    [0, 45, 26],\n",
    "    [1, 160, 29],\n",
    "    [2, 190, 25],\n",
    "    [3, 75, 28],\n",
    "    [0, 55, 24],\n",
    "    [1, 145, 27],\n",
    "])\n",
    "\n",
    "# Prix du manioc (en milliers de FC par sac)\n",
    "y_prix = np.array([45, 35, 30, 38, 46, 34, 31, 39, 44, 36])\n",
    "\n",
    "# Normaliser\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_manioc_scaled = scaler_X.fit_transform(X_manioc)\n",
    "y_prix_scaled = scaler_y.fit_transform(y_prix.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "model_manioc = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation='relu', input_shape=(3,)),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(1)  # Pas d'activation pour la r√©gression\n",
    "])\n",
    "\n",
    "model_manioc.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Entra√Æner\n",
    "history_manioc = model_manioc.fit(\n",
    "    X_manioc_scaled, y_prix_scaled,\n",
    "    epochs=200,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Pr√©dire\n",
    "predictions_scaled = model_manioc.predict(X_manioc_scaled, verbose=0)\n",
    "predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"Pr√©dictions de prix du manioc:\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(y_prix)):\n",
    "    print(f\"R√©el: {y_prix[i]:.0f} FC | Pr√©dit: {predictions[i][0]:.0f} FC | Erreur: {abs(y_prix[i] - predictions[i][0]):.0f} FC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Introduction √† PyTorch\n",
    "\n",
    "**PyTorch** est une biblioth√®que d√©velopp√©e par Facebook (Meta) pour le Deep Learning.\n",
    "\n",
    "### Avantages\n",
    "- ‚úÖ Plus pythonique et intuitif\n",
    "- ‚úÖ Excellent pour la recherche\n",
    "- ‚úÖ D√©bogage plus facile\n",
    "- ‚úÖ Graphes dynamiques\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install torch torchvision\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de PyTorch\n",
    "!pip install torch torchvision -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√™me exemple avec PyTorch\n",
    "\n",
    "# D√©finir le mod√®le\n",
    "class ManiocPriceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ManiocPriceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "model_pytorch = ManiocPriceModel()\n",
    "\n",
    "# D√©finir la fonction de perte et l'optimiseur\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_pytorch.parameters(), lr=0.01)\n",
    "\n",
    "# Convertir les donn√©es en tenseurs PyTorch\n",
    "X_tensor = torch.FloatTensor(X_manioc_scaled)\n",
    "y_tensor = torch.FloatTensor(y_prix_scaled).reshape(-1, 1)\n",
    "\n",
    "# Entra√Æner\n",
    "losses = []\n",
    "for epoch in range(200):\n",
    "    # Forward pass\n",
    "    predictions = model_pytorch(X_tensor)\n",
    "    loss = criterion(predictions, y_tensor)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "\n",
    "# Visualiser l'entra√Ænement\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('√âpoque', fontsize=12)\n",
    "plt.ylabel('Perte (MSE)', fontsize=12)\n",
    "plt.title('Entra√Ænement avec PyTorch', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Perte finale: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison TensorFlow vs PyTorch\n",
    "\n",
    "| Aspect | TensorFlow | PyTorch |\n",
    "|--------|------------|----------|\n",
    "| **Facilit√© d'apprentissage** | ‚≠ê‚≠ê‚≠ê‚≠ê (avec Keras) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Production** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Recherche** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **D√©bogage** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Communaut√©** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "**Recommandation pour d√©butants :** TensorFlow avec Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercice 3 : Cr√©er votre premier r√©seau\n",
    "\n",
    "Cr√©ez un r√©seau de neurones pour pr√©dire le rendement agricole.\n",
    "\n",
    "**Donn√©es :**\n",
    "- Entr√©es : [Pluie (mm), Engrais (kg), Temp√©rature (¬∞C)]\n",
    "- Sortie : Rendement (tonnes/hectare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3 : Pr√©diction de rendement agricole\n",
    "\n",
    "X_agriculture = np.array([\n",
    "    [800, 50, 25],\n",
    "    [1200, 80, 27],\n",
    "    [600, 30, 24],\n",
    "    [1000, 60, 26],\n",
    "    [1400, 90, 28],\n",
    "    [700, 40, 25],\n",
    "])\n",
    "\n",
    "y_rendement = np.array([3.5, 5.2, 2.8, 4.1, 5.8, 3.2])\n",
    "\n",
    "# TODO: Cr√©ez et entra√Ænez votre mod√®le\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 5 : Probl√®mes et solutions <a id=\"chapitre-5\"></a>\n",
    "\n",
    "## 5.1 Surapprentissage (Overfitting)\n",
    "\n",
    "Le **surapprentissage** se produit quand le mod√®le apprend \"par c≈ìur\" les donn√©es d'entra√Ænement mais ne g√©n√©ralise pas bien.\n",
    "\n",
    "### Signes de surapprentissage\n",
    "- ‚úÖ Excellente performance sur les donn√©es d'entra√Ænement\n",
    "- ‚ùå Mauvaise performance sur les donn√©es de test\n",
    "- üìà √âcart croissant entre perte d'entra√Ænement et de validation\n",
    "\n",
    "### Causes\n",
    "1. Mod√®le trop complexe (trop de param√®tres)\n",
    "2. Pas assez de donn√©es d'entra√Ænement\n",
    "3. Entra√Ænement trop long\n",
    "\n",
    "## 5.2 Sous-apprentissage (Underfitting)\n",
    "\n",
    "Le **sous-apprentissage** se produit quand le mod√®le est trop simple et ne capture pas les patterns.\n",
    "\n",
    "### Signes de sous-apprentissage\n",
    "- ‚ùå Mauvaise performance sur l'entra√Ænement\n",
    "- ‚ùå Mauvaise performance sur le test\n",
    "- üìâ Perte √©lev√©e qui ne diminue pas\n",
    "\n",
    "## 5.3 Solutions\n",
    "\n",
    "### 1. Validation crois√©e (Cross-validation)\n",
    "\n",
    "Diviser les donn√©es en 3 ensembles :\n",
    "- **Entra√Ænement (70%)** : Pour apprendre\n",
    "- **Validation (15%)** : Pour ajuster les hyperparam√®tres\n",
    "- **Test (15%)** : Pour √©valuer la performance finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Split des donn√©es\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cr√©er des donn√©es synth√©tiques\n",
    "X_data = np.random.randn(1000, 5)\n",
    "y_data = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# Split 1 : S√©parer test (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Split 2 : S√©parer validation (15% du reste)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42  # 0.176 * 0.85 ‚âà 0.15\n",
    ")\n",
    "\n",
    "print(f\"Taille de l'ensemble d'entra√Ænement: {len(X_train)} ({len(X_train)/len(X_data)*100:.1f}%)\")\n",
    "print(f\"Taille de l'ensemble de validation: {len(X_val)} ({len(X_val)/len(X_data)*100:.1f}%)\")\n",
    "print(f\"Taille de l'ensemble de test: {len(X_test)} ({len(X_test)/len(X_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dropout\n",
    "\n",
    "Le **dropout** d√©sactive al√©atoirement des neurones pendant l'entra√Ænement pour √©viter le surapprentissage.\n",
    "\n",
    "**Analogie :** C'est comme √©tudier avec diff√©rents groupes d'amis. Vous ne d√©pendez pas toujours des m√™mes personnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le avec Dropout\n",
    "\n",
    "model_dropout = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(5,)),\n",
    "    keras.layers.Dropout(0.3),  # D√©sactive 30% des neurones\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_dropout.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. R√©gularisation L1/L2\n",
    "\n",
    "La **r√©gularisation** p√©nalise les poids trop grands pour simplifier le mod√®le.\n",
    "\n",
    "- **L1** : Pousse certains poids vers 0 (s√©lection de features)\n",
    "- **L2** : R√©duit tous les poids (plus courant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le avec r√©gularisation L2\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model_regularized = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', \n",
    "                      kernel_regularizer=regularizers.l2(0.01),\n",
    "                      input_shape=(5,)),\n",
    "    keras.layers.Dense(32, activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_regularized.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Mod√®le avec r√©gularisation L2:\")\n",
    "model_regularized.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Early Stopping\n",
    "\n",
    "Arr√™ter l'entra√Ænement quand la performance sur la validation ne s'am√©liore plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Attendre 10 √©poques sans am√©lioration\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Entra√Æner avec early stopping\n",
    "history_early = model_dropout.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Entra√Ænement arr√™t√© √† l'√©poque {len(history_early.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercice 4 : Combattre le surapprentissage\n",
    "\n",
    "Cr√©ez deux mod√®les et comparez-les :\n",
    "1. Un mod√®le simple sans r√©gularisation\n",
    "2. Un mod√®le avec Dropout et Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 4 : Comparer les mod√®les\n",
    "\n",
    "# TODO: Cr√©ez deux mod√®les et comparez leurs performances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 6 : Projet final - Classification de fruits <a id=\"chapitre-6\"></a>\n",
    "\n",
    "## üéØ Objectif du projet\n",
    "\n",
    "Cr√©er un r√©seau de neurones pour classifier des fruits locaux congolais :\n",
    "- üçå **Banane**\n",
    "- ü•≠ **Mangue**\n",
    "- üçç **Ananas**\n",
    "\n",
    "## √âtapes du projet\n",
    "\n",
    "1. Cr√©er un dataset synth√©tique de features\n",
    "2. Construire un r√©seau de neurones\n",
    "3. Entra√Æner le mod√®le\n",
    "4. √âvaluer les performances\n",
    "5. Faire des pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 1 : Cr√©er le dataset\n",
    "\n",
    "# Features : [Longueur (cm), Largeur (cm), Poids (g), Couleur (0-1), Texture (0-1)]\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Bananes : allong√©es, jaunes\n",
    "bananes = np.random.randn(100, 5) * np.array([2, 1, 20, 0.1, 0.1]) + np.array([18, 4, 120, 0.8, 0.3])\n",
    "\n",
    "# Mangues : ovales, orange-rouge\n",
    "mangues = np.random.randn(100, 5) * np.array([2, 2, 30, 0.1, 0.1]) + np.array([12, 8, 250, 0.6, 0.5])\n",
    "\n",
    "# Ananas : gros, rugueux\n",
    "ananas = np.random.randn(100, 5) * np.array([3, 3, 50, 0.1, 0.1]) + np.array([25, 12, 1000, 0.4, 0.8])\n",
    "\n",
    "# Combiner les donn√©es\n",
    "X_fruits = np.vstack([bananes, mangues, ananas])\n",
    "y_fruits = np.hstack([\n",
    "    np.zeros(100),  # Bananes = 0\n",
    "    np.ones(100),   # Mangues = 1\n",
    "    np.full(100, 2) # Ananas = 2\n",
    "])\n",
    "\n",
    "# M√©langer\n",
    "indices = np.random.permutation(300)\n",
    "X_fruits = X_fruits[indices]\n",
    "y_fruits = y_fruits[indices]\n",
    "\n",
    "print(\"Dataset cr√©√©:\")\n",
    "print(f\"- Nombre d'exemples: {len(X_fruits)}\")\n",
    "print(f\"- Features par exemple: {X_fruits.shape[1]}\")\n",
    "print(f\"- Classes: 3 (Banane, Mangue, Ananas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les donn√©es\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Graphique 1 : Longueur vs Largeur\n",
    "ax1 = fig.add_subplot(121)\n",
    "colors = ['yellow', 'orange', 'green']\n",
    "labels = ['Banane', 'Mangue', 'Ananas']\n",
    "\n",
    "for i in range(3):\n",
    "    mask = y_fruits == i\n",
    "    ax1.scatter(X_fruits[mask, 0], X_fruits[mask, 1], \n",
    "               c=colors[i], label=labels[i], alpha=0.6, s=50)\n",
    "\n",
    "ax1.set_xlabel('Longueur (cm)', fontsize=12)\n",
    "ax1.set_ylabel('Largeur (cm)', fontsize=12)\n",
    "ax1.set_title('Dimensions des fruits', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 2 : Poids vs Texture\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "for i in range(3):\n",
    "    mask = y_fruits == i\n",
    "    ax2.scatter(X_fruits[mask, 2], X_fruits[mask, 4], \n",
    "               c=colors[i], label=labels[i], alpha=0.6, s=50)\n",
    "\n",
    "ax2.set_xlabel('Poids (g)', fontsize=12)\n",
    "ax2.set_ylabel('Texture', fontsize=12)\n",
    "ax2.set_title('Poids et texture des fruits', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 2 : Pr√©parer les donn√©es\n",
    "\n",
    "# Split train/test\n",
    "X_train_fruits, X_test_fruits, y_train_fruits, y_test_fruits = train_test_split(\n",
    "    X_fruits, y_fruits, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normaliser\n",
    "scaler_fruits = StandardScaler()\n",
    "X_train_fruits_scaled = scaler_fruits.fit_transform(X_train_fruits)\n",
    "X_test_fruits_scaled = scaler_fruits.transform(X_test_fruits)\n",
    "\n",
    "# Convertir les labels en one-hot encoding\n",
    "y_train_fruits_cat = keras.utils.to_categorical(y_train_fruits, 3)\n",
    "y_test_fruits_cat = keras.utils.to_categorical(y_test_fruits, 3)\n",
    "\n",
    "print(\"Donn√©es pr√©par√©es:\")\n",
    "print(f\"- Entra√Ænement: {len(X_train_fruits)} exemples\")\n",
    "print(f\"- Test: {len(X_test_fruits)} exemples\")\n",
    "print(f\"- Shape des labels: {y_train_fruits_cat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 3 : Construire le mod√®le\n",
    "\n",
    "model_fruits = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(5,)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')  # 3 classes\n",
    "])\n",
    "\n",
    "model_fruits.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Architecture du mod√®le:\")\n",
    "model_fruits.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 4 : Entra√Æner le mod√®le\n",
    "\n",
    "early_stop_fruits = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history_fruits = model_fruits.fit(\n",
    "    X_train_fruits_scaled, y_train_fruits_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop_fruits],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Visualiser l'entra√Ænement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Perte\n",
    "axes[0].plot(history_fruits.history['loss'], label='Entra√Ænement', linewidth=2)\n",
    "axes[0].plot(history_fruits.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0].set_xlabel('√âpoque', fontsize=12)\n",
    "axes[0].set_ylabel('Perte', fontsize=12)\n",
    "axes[0].set_title('√âvolution de la perte', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Pr√©cision\n",
    "axes[1].plot(history_fruits.history['accuracy'], label='Entra√Ænement', linewidth=2)\n",
    "axes[1].plot(history_fruits.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('√âpoque', fontsize=12)\n",
    "axes[1].set_ylabel('Pr√©cision', fontsize=12)\n",
    "axes[1].set_title('√âvolution de la pr√©cision', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Entra√Ænement termin√© en {len(history_fruits.history['loss'])} √©poques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 5 : √âvaluer le mod√®le\n",
    "\n",
    "test_loss, test_accuracy = model_fruits.evaluate(\n",
    "    X_test_fruits_scaled, y_test_fruits_cat, verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä R√âSULTATS FINAUX\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Perte sur le test: {test_loss:.4f}\")\n",
    "print(f\"Pr√©cision sur le test: {test_accuracy * 100:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_proba = model_fruits.predict(X_test_fruits_scaled, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test_fruits, y_pred)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Banane', 'Mangue', 'Ananas'],\n",
    "            yticklabels=['Banane', 'Mangue', 'Ananas'])\n",
    "plt.xlabel('Pr√©diction', fontsize=12)\n",
    "plt.ylabel('V√©rit√©', fontsize=12)\n",
    "plt.title('Matrice de confusion - Classification de fruits', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(y_test_fruits, y_pred, \n",
    "                          target_names=['Banane', 'Mangue', 'Ananas']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 6 : Faire des pr√©dictions sur de nouveaux fruits\n",
    "\n",
    "# Nouveaux fruits √† classifier\n",
    "nouveaux_fruits = np.array([\n",
    "    [20, 4, 130, 0.85, 0.25],  # Devrait √™tre une banane\n",
    "    [11, 7, 240, 0.55, 0.48],  # Devrait √™tre une mangue\n",
    "    [26, 13, 1050, 0.38, 0.82], # Devrait √™tre un ananas\n",
    "])\n",
    "\n",
    "# Normaliser\n",
    "nouveaux_fruits_scaled = scaler_fruits.transform(nouveaux_fruits)\n",
    "\n",
    "# Pr√©dire\n",
    "predictions = model_fruits.predict(nouveaux_fruits_scaled, verbose=0)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "fruit_names = ['Banane', 'Mangue', 'Ananas']\n",
    "\n",
    "print(\"\\nüçé PR√âDICTIONS SUR DE NOUVEAUX FRUITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    predicted_class = np.argmax(pred)\n",
    "    confidence = pred[predicted_class] * 100\n",
    "    \n",
    "    print(f\"\\nFruit {i+1}:\")\n",
    "    print(f\"  Caract√©ristiques: {nouveaux_fruits[i]}\")\n",
    "    print(f\"  Pr√©diction: {fruit_names[predicted_class]}\")\n",
    "    print(f\"  Confiance: {confidence:.2f}%\")\n",
    "    print(f\"  Probabilit√©s:\")\n",
    "    for j, prob in enumerate(pred):\n",
    "        print(f\"    - {fruit_names[j]}: {prob*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercice 5 : Am√©liorer le mod√®le\n",
    "\n",
    "Essayez d'am√©liorer la performance du mod√®le en :\n",
    "1. Ajoutant plus de couches\n",
    "2. Modifiant le nombre de neurones\n",
    "3. Testant diff√©rentes fonctions d'activation\n",
    "4. Ajustant le dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5 : Am√©liorez le mod√®le\n",
    "\n",
    "# TODO: Cr√©ez une version am√©lior√©e du mod√®le\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì R√©sum√© du module\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **Deep Learning vs ML classique**\n",
    "   - Extraction automatique de features\n",
    "   - Besoin de grandes quantit√©s de donn√©es\n",
    "   - Applications en RDC\n",
    "\n",
    "2. **R√©seaux de neurones**\n",
    "   - Architecture (couches, neurones)\n",
    "   - Fonctions d'activation (ReLU, Sigmoid, Softmax)\n",
    "   - Propagation avant et arri√®re\n",
    "\n",
    "3. **Entra√Ænement**\n",
    "   - Fonctions de perte\n",
    "   - Backpropagation\n",
    "   - Optimiseurs (Adam, SGD)\n",
    "\n",
    "4. **Frameworks**\n",
    "   - TensorFlow/Keras\n",
    "   - PyTorch\n",
    "   - Comparaison et choix\n",
    "\n",
    "5. **Probl√®mes courants**\n",
    "   - Surapprentissage et solutions\n",
    "   - Dropout, r√©gularisation\n",
    "   - Early stopping\n",
    "\n",
    "6. **Projet pratique**\n",
    "   - Classification multi-classes\n",
    "   - √âvaluation de performance\n",
    "   - Matrice de confusion\n",
    "\n",
    "### Prochaines √©tapes\n",
    "\n",
    "1. **R√©seaux convolutifs (CNN)**\n",
    "   - Traitement d'images\n",
    "   - D√©tection d'objets\n",
    "\n",
    "2. **R√©seaux r√©currents (RNN/LSTM)**\n",
    "   - S√©ries temporelles\n",
    "   - Traitement du langage\n",
    "\n",
    "3. **Transfer Learning**\n",
    "   - Utiliser des mod√®les pr√©-entra√Æn√©s\n",
    "   - Fine-tuning\n",
    "\n",
    "4. **D√©ploiement**\n",
    "   - API REST\n",
    "   - Applications mobiles\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Ressources suppl√©mentaires\n",
    "\n",
    "### Cours en ligne\n",
    "- [Deep Learning Specialization - Coursera](https://www.coursera.org/specializations/deep-learning)\n",
    "- [Fast.ai - Practical Deep Learning](https://www.fast.ai/)\n",
    "- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)\n",
    "\n",
    "### Livres\n",
    "- \"Deep Learning\" - Ian Goodfellow\n",
    "- \"Hands-On Machine Learning\" - Aur√©lien G√©ron\n",
    "- \"Neural Networks and Deep Learning\" - Michael Nielsen (gratuit)\n",
    "\n",
    "### Communaut√©s\n",
    "- [Kaggle](https://www.kaggle.com/) - Comp√©titions et datasets\n",
    "- [Papers With Code](https://paperswithcode.com/) - Derni√®res recherches\n",
    "- [Reddit r/MachineLearning](https://www.reddit.com/r/MachineLearning/)\n",
    "\n",
    "---\n",
    "\n",
    "**F√©licitations ! Vous avez termin√© le module Deep Learning ! üéâ**\n",
    "\n",
    "*Continuez √† pratiquer et √† explorer. Le Deep Learning est un domaine en constante √©volution !*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
