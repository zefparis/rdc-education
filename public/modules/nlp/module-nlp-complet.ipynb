{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module NLP - Formation Compl√®te\n",
    "\n",
    "## Plateforme IA-Solution RDC\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objectifs du module\n",
    "\n",
    "√Ä la fin de ce module, vous serez capable de :\n",
    "- Comprendre les bases du traitement du langage naturel\n",
    "- Pr√©traiter et nettoyer des textes\n",
    "- Utiliser des mod√®les classiques (TF-IDF) et modernes (Transformers)\n",
    "- Cr√©er des applications NLP pour la RDC\n",
    "- Construire un chatbot simple\n",
    "\n",
    "**Niveau :** Interm√©diaire  \n",
    "**Pr√©requis :** Python, Machine Learning, Deep Learning  \n",
    "**Dur√©e :** 6 semaines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Table des mati√®res\n",
    "\n",
    "1. [Introduction au NLP](#chapitre-1)\n",
    "2. [Pr√©traitement du texte](#chapitre-2)\n",
    "3. [Word Embeddings](#chapitre-3)\n",
    "4. [Transformers](#chapitre-4)\n",
    "5. [Applications en RDC](#chapitre-5)\n",
    "6. [Projet final : Chatbot](#chapitre-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des biblioth√®ques\n",
    "!pip install nltk spacy transformers torch scikit-learn -q\n",
    "!python -m spacy download fr_core_news_sm -q\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques install√©es !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SpaCy\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"‚úÖ Imports termin√©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 1 : Introduction au NLP <a id=\"chapitre-1\"></a>\n",
    "\n",
    "## 1.1 Qu'est-ce que le NLP ?\n",
    "\n",
    "Le **NLP** (Natural Language Processing) ou **TALN** (Traitement Automatique du Langage Naturel) est le domaine de l'IA qui permet aux ordinateurs de comprendre, interpr√©ter et g√©n√©rer du langage humain.\n",
    "\n",
    "### Applications courantes\n",
    "\n",
    "1. **Traduction automatique** : Google Translate\n",
    "2. **Assistants vocaux** : Siri, Alexa\n",
    "3. **Analyse de sentiments** : Avis clients\n",
    "4. **Chatbots** : Service client\n",
    "5. **R√©sum√© automatique** : Articles de presse\n",
    "6. **Correction orthographique** : Word, Gmail\n",
    "\n",
    "## 1.2 Applications en RDC\n",
    "\n",
    "### üìö **√âducation**\n",
    "- Chatbots pour r√©pondre aux questions d'√©tudiants\n",
    "- Correction automatique de devoirs\n",
    "- Traduction de cours en langues locales\n",
    "\n",
    "### üèõÔ∏è **Administration**\n",
    "- Analyse de documents administratifs\n",
    "- Classification automatique de courriers\n",
    "- Extraction d'informations\n",
    "\n",
    "### üí¨ **Communication**\n",
    "- Traduction fran√ßais ‚Üî lingala/swahili\n",
    "- Analyse de r√©seaux sociaux\n",
    "- Mod√©ration de contenus\n",
    "\n",
    "### üè• **Sant√©**\n",
    "- Analyse de dossiers m√©dicaux\n",
    "- Chatbot de premiers secours\n",
    "- Extraction de sympt√¥mes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 2 : Pr√©traitement du texte <a id=\"chapitre-2\"></a>\n",
    "\n",
    "## 2.1 √âtapes du pr√©traitement\n",
    "\n",
    "```\n",
    "Texte brut\n",
    "    ‚Üì\n",
    "1. Nettoyage (ponctuation, chiffres)\n",
    "    ‚Üì\n",
    "2. Tokenisation (d√©coupage en mots)\n",
    "    ‚Üì\n",
    "3. Normalisation (minuscules)\n",
    "    ‚Üì\n",
    "4. Suppression des stopwords\n",
    "    ‚Üì\n",
    "5. Lemmatisation/Stemming\n",
    "    ‚Üì\n",
    "Texte pr√©trait√©\n",
    "```\n",
    "\n",
    "## 2.2 Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de texte\n",
    "texte = \"\"\"Bonjour ! Je m'appelle Jean et j'habite √† Kinshasa, RDC. \n",
    "J'√©tudie l'intelligence artificielle depuis 2023. C'est passionnant !!!\"\"\"\n",
    "\n",
    "print(\"üìù TEXTE ORIGINAL\")\n",
    "print(\"=\" * 60)\n",
    "print(texte)\n",
    "\n",
    "# Fonction de nettoyage\n",
    "def nettoyer_texte(texte):\n",
    "    \"\"\"Nettoie un texte\"\"\"\n",
    "    # Minuscules\n",
    "    texte = texte.lower()\n",
    "    \n",
    "    # Supprimer la ponctuation\n",
    "    texte = texte.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Supprimer les chiffres\n",
    "    texte = re.sub(r'\\d+', '', texte)\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    texte = re.sub(r'\\s+', ' ', texte).strip()\n",
    "    \n",
    "    return texte\n",
    "\n",
    "texte_nettoye = nettoyer_texte(texte)\n",
    "\n",
    "print(\"\\nüßπ TEXTE NETTOY√â\")\n",
    "print(\"=\" * 60)\n",
    "print(texte_nettoye)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation en phrases\n",
    "phrases = sent_tokenize(texte, language='french')\n",
    "\n",
    "print(\"üìÑ TOKENISATION EN PHRASES\")\n",
    "print(\"=\" * 60)\n",
    "for i, phrase in enumerate(phrases, 1):\n",
    "    print(f\"{i}. {phrase}\")\n",
    "\n",
    "# Tokenisation en mots\n",
    "mots = word_tokenize(texte_nettoye, language='french')\n",
    "\n",
    "print(\"\\nüî§ TOKENISATION EN MOTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Nombre de mots : {len(mots)}\")\n",
    "print(f\"Mots : {mots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Suppression des stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les stopwords fran√ßais\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "print(\"üö´ STOPWORDS FRAN√áAIS (exemples)\")\n",
    "print(\"=\" * 60)\n",
    "print(list(stop_words)[:20])\n",
    "\n",
    "# Filtrer les stopwords\n",
    "mots_filtres = [mot for mot in mots if mot not in stop_words]\n",
    "\n",
    "print(\"\\n‚úÖ MOTS APR√àS FILTRAGE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Avant : {len(mots)} mots\")\n",
    "print(f\"Apr√®s : {len(mots_filtres)} mots\")\n",
    "print(f\"Mots conserv√©s : {mots_filtres}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Lemmatisation avec SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de lemmatisation\n",
    "texte_exemple = \"Les √©tudiants √©tudient l'intelligence artificielle dans les universit√©s.\"\n",
    "\n",
    "doc = nlp(texte_exemple)\n",
    "\n",
    "print(\"üîç LEMMATISATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Mot':<20} {'Lemme':<20} {'POS'}\")\n",
    "print(\"-\" * 60)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<20} {token.lemma_:<20} {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Vectorisation\n",
    "\n",
    "### Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus d'exemple\n",
    "corpus = [\n",
    "    \"L'√©ducation est importante en RDC\",\n",
    "    \"L'intelligence artificielle transforme l'√©ducation\",\n",
    "    \"La RDC investit dans l'√©ducation\"\n",
    "]\n",
    "\n",
    "# Bag-of-Words\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform(corpus)\n",
    "\n",
    "print(\"üìä BAG-OF-WORDS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Vocabulaire : {vectorizer_bow.get_feature_names_out()}\")\n",
    "print(f\"\\nMatrice BoW :\")\n",
    "print(X_bow.toarray())\n",
    "\n",
    "# Afficher sous forme de DataFrame\n",
    "df_bow = pd.DataFrame(\n",
    "    X_bow.toarray(),\n",
    "    columns=vectorizer_bow.get_feature_names_out()\n",
    ")\n",
    "print(\"\\nDataFrame :\")\n",
    "print(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(corpus)\n",
    "\n",
    "print(\"üìà TF-IDF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_tfidf = pd.DataFrame(\n",
    "    X_tfidf.toarray(),\n",
    "    columns=vectorizer_tfidf.get_feature_names_out()\n",
    ")\n",
    "print(df_tfidf)\n",
    "\n",
    "# Similarit√© cosinus\n",
    "similarity = cosine_similarity(X_tfidf)\n",
    "\n",
    "print(\"\\nüîó SIMILARIT√â ENTRE DOCUMENTS\")\n",
    "print(\"=\" * 60)\n",
    "print(pd.DataFrame(similarity, \n",
    "                   columns=[f\"Doc {i+1}\" for i in range(len(corpus))],\n",
    "                   index=[f\"Doc {i+1}\" for i in range(len(corpus))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercice 1 : Pr√©traitement\n",
    "\n",
    "Pr√©traitez le texte suivant :\n",
    "```\n",
    "\"Le Congo-Kinshasa compte plus de 90 millions d'habitants en 2023. \n",
    "C'est un pays riche en ressources naturelles !!!\"\n",
    "```\n",
    "\n",
    "1. Nettoyez le texte\n",
    "2. Tokenisez en mots\n",
    "3. Supprimez les stopwords\n",
    "4. Lemmatisez avec SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1 : √Ä vous de jouer !\n",
    "\n",
    "texte_exercice = \"\"\"Le Congo-Kinshasa compte plus de 90 millions d'habitants en 2023. \n",
    "C'est un pays riche en ressources naturelles !!!\"\"\"\n",
    "\n",
    "# TODO: Pr√©traitez le texte\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 3 : Word Embeddings <a id=\"chapitre-3\"></a>\n",
    "\n",
    "## 3.1 Qu'est-ce qu'un Word Embedding ?\n",
    "\n",
    "Un **word embedding** est une repr√©sentation vectorielle dense d'un mot qui capture son sens s√©mantique.\n",
    "\n",
    "### Diff√©rence avec BoW/TF-IDF\n",
    "\n",
    "| M√©thode | Repr√©sentation | Taille | S√©mantique |\n",
    "|---------|----------------|--------|------------|\n",
    "| **BoW/TF-IDF** | Sparse (creuse) | Grande | Non |\n",
    "| **Word Embeddings** | Dense | Petite (50-300) | Oui |\n",
    "\n",
    "### Mod√®les populaires\n",
    "\n",
    "- **Word2Vec** (Google, 2013)\n",
    "- **GloVe** (Stanford, 2014)\n",
    "- **FastText** (Facebook, 2016)\n",
    "\n",
    "## 3.2 Utiliser des embeddings pr√©-entra√Æn√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser SpaCy pour les embeddings\n",
    "mots_test = [\"roi\", \"reine\", \"homme\", \"femme\", \"√©cole\", \"universit√©\"]\n",
    "\n",
    "print(\"üî¢ WORD EMBEDDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for mot in mots_test:\n",
    "    token = nlp(mot)\n",
    "    vector = token[0].vector\n",
    "    print(f\"{mot:<15} ‚Üí Vecteur de dimension {len(vector)}\")\n",
    "    print(f\"  Premiers √©l√©ments : {vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Similarit√© s√©mantique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la similarit√© entre mots\n",
    "def similarite_mots(mot1, mot2):\n",
    "    \"\"\"Calcule la similarit√© entre deux mots\"\"\"\n",
    "    token1 = nlp(mot1)\n",
    "    token2 = nlp(mot2)\n",
    "    return token1.similarity(token2)\n",
    "\n",
    "print(\"üîó SIMILARIT√â S√âMANTIQUE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "paires = [\n",
    "    (\"roi\", \"reine\"),\n",
    "    (\"homme\", \"femme\"),\n",
    "    (\"√©cole\", \"universit√©\"),\n",
    "    (\"Kinshasa\", \"Lubumbashi\"),\n",
    "    (\"roi\", \"√©cole\"),\n",
    "]\n",
    "\n",
    "for mot1, mot2 in paires:\n",
    "    sim = similarite_mots(mot1, mot2)\n",
    "    print(f\"{mot1:<15} ‚Üî {mot2:<15} : {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercice 2 : Word Embeddings\n",
    "\n",
    "1. Calculez la similarit√© entre : \"m√©decin\" et \"infirmier\"\n",
    "2. Trouvez quel mot est le plus similaire √† \"enseignant\" : \"professeur\" ou \"banane\"\n",
    "3. Calculez la similarit√© entre \"Kinshasa\" et \"capitale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2 : √Ä vous de jouer !\n",
    "\n",
    "# TODO: Calculez les similarit√©s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 4 : Transformers <a id=\"chapitre-4\"></a>\n",
    "\n",
    "## 4.1 Qu'est-ce qu'un Transformer ?\n",
    "\n",
    "Les **Transformers** sont des mod√®les de deep learning bas√©s sur le m√©canisme d'**attention** qui r√©volutionnent le NLP.\n",
    "\n",
    "### Mod√®les populaires\n",
    "\n",
    "| Mod√®le | Cr√©ateur | Ann√©e | Usage |\n",
    "|--------|----------|-------|-------|\n",
    "| **BERT** | Google | 2018 | Compr√©hension |\n",
    "| **GPT** | OpenAI | 2018 | G√©n√©ration |\n",
    "| **T5** | Google | 2019 | Universel |\n",
    "| **RoBERTa** | Facebook | 2019 | Compr√©hension |\n",
    "| **CamemBERT** | INRIA | 2019 | Fran√ßais |\n",
    "\n",
    "## 4.2 Utiliser Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de sentiments\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "textes = [\n",
    "    \"J'adore ce cours d'intelligence artificielle !\",\n",
    "    \"Ce module est tr√®s difficile et ennuyeux.\",\n",
    "    \"L'√©ducation en RDC s'am√©liore chaque ann√©e.\"\n",
    "]\n",
    "\n",
    "print(\"üòä ANALYSE DE SENTIMENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for texte in textes:\n",
    "    result = sentiment_analyzer(texte)[0]\n",
    "    print(f\"\\nTexte : {texte}\")\n",
    "    print(f\"Sentiment : {result['label']} (confiance: {result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification de texte\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "texte = \"L'universit√© de Kinshasa propose des cours d'intelligence artificielle.\"\n",
    "categories = [\"√©ducation\", \"sant√©\", \"agriculture\", \"technologie\"]\n",
    "\n",
    "result = classifier(texte, categories)\n",
    "\n",
    "print(\"\\nüè∑Ô∏è CLASSIFICATION ZERO-SHOT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Texte : {texte}\\n\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"{label:<15} : {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traduction fran√ßais ‚Üí anglais\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "\n",
    "textes_fr = [\n",
    "    \"Bonjour, comment allez-vous ?\",\n",
    "    \"L'intelligence artificielle transforme l'√©ducation en RDC.\",\n",
    "    \"Je suis √©tudiant √† Kinshasa.\"\n",
    "]\n",
    "\n",
    "print(\"üåç TRADUCTION FRAN√áAIS ‚Üí ANGLAIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for texte_fr in textes_fr:\n",
    "    result = translator(texte_fr)[0]\n",
    "    print(f\"\\nFR : {texte_fr}\")\n",
    "    print(f\"EN : {result['translation_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercice 3 : Transformers\n",
    "\n",
    "1. Analysez le sentiment de : \"Ce tutoriel est excellent et tr√®s utile !\"\n",
    "2. Classifiez : \"Le paludisme est une maladie grave en Afrique.\"\n",
    "3. Traduisez en anglais : \"J'√©tudie le machine learning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3 : √Ä vous de jouer !\n",
    "\n",
    "# TODO: Utilisez les pipelines Hugging Face\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 5 : Applications en RDC <a id=\"chapitre-5\"></a>\n",
    "\n",
    "## 5.1 Chatbot √©ducatif simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot bas√© sur des r√®gles\n",
    "class ChatbotEducatif:\n",
    "    def __init__(self):\n",
    "        self.reponses = {\n",
    "            \"bonjour\": \"Bonjour ! Comment puis-je vous aider aujourd'hui ?\",\n",
    "            \"ia\": \"L'intelligence artificielle est la simulation de l'intelligence humaine par des machines.\",\n",
    "            \"machine learning\": \"Le machine learning est une branche de l'IA qui permet aux ordinateurs d'apprendre √† partir de donn√©es.\",\n",
    "            \"python\": \"Python est un langage de programmation populaire pour l'IA et le data science.\",\n",
    "            \"cours\": \"Nous proposons des cours en Data Science, Deep Learning, IA G√©n√©rative, MLOps et NLP.\",\n",
    "            \"merci\": \"De rien ! N'h√©sitez pas si vous avez d'autres questions.\",\n",
    "            \"au revoir\": \"Au revoir ! Bon apprentissage !\"\n",
    "        }\n",
    "    \n",
    "    def repondre(self, message):\n",
    "        \"\"\"G√©n√®re une r√©ponse\"\"\"\n",
    "        message = message.lower()\n",
    "        \n",
    "        for mot_cle, reponse in self.reponses.items():\n",
    "            if mot_cle in message:\n",
    "                return reponse\n",
    "        \n",
    "        return \"Je ne comprends pas votre question. Pouvez-vous reformuler ?\"\n",
    "\n",
    "# Test du chatbot\n",
    "chatbot = ChatbotEducatif()\n",
    "\n",
    "print(\"ü§ñ CHATBOT √âDUCATIF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "questions = [\n",
    "    \"Bonjour\",\n",
    "    \"Qu'est-ce que l'IA ?\",\n",
    "    \"Parlez-moi du machine learning\",\n",
    "    \"Quels cours proposez-vous ?\",\n",
    "    \"Merci beaucoup\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    reponse = chatbot.repondre(question)\n",
    "    print(f\"\\nüë§ Utilisateur : {question}\")\n",
    "    print(f\"ü§ñ Chatbot : {reponse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Analyse de textes administratifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction d'entit√©s nomm√©es\n",
    "texte_admin = \"\"\"Le Minist√®re de l'√âducation de la R√©publique D√©mocratique du Congo \n",
    "annonce l'ouverture de 50 nouvelles √©coles √† Kinshasa en 2025. \n",
    "Le ministre Jean Mukendi a d√©clar√© que le budget allou√© est de 10 millions de dollars.\"\"\"\n",
    "\n",
    "doc = nlp(texte_admin)\n",
    "\n",
    "print(\"üèõÔ∏è EXTRACTION D'ENTIT√âS NOMM√âES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Texte : {texte_admin}\\n\")\n",
    "print(f\"{'Entit√©':<30} {'Type'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<30} {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercice 4 : Applications RDC\n",
    "\n",
    "1. Am√©liorez le chatbot avec 5 nouvelles questions/r√©ponses\n",
    "2. Extrayez les entit√©s de ce texte :\n",
    "   \"L'Universit√© de Lubumbashi organise une conf√©rence sur l'IA le 15 mars 2025.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 4 : √Ä vous de jouer !\n",
    "\n",
    "# TODO: Am√©liorez le chatbot et extrayez les entit√©s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapitre 6 : Projet final <a id=\"chapitre-6\"></a>\n",
    "\n",
    "## üéØ Objectif\n",
    "\n",
    "Cr√©er un **chatbot √©ducatif intelligent** pour r√©pondre aux questions d'√©tudiants.\n",
    "\n",
    "## Sp√©cifications\n",
    "\n",
    "1. **Base de connaissances** : 20+ questions/r√©ponses\n",
    "2. **Pr√©traitement** : Nettoyage et normalisation\n",
    "3. **Similarit√©** : TF-IDF + cosine similarity\n",
    "4. **R√©ponse** : Trouver la meilleure r√©ponse\n",
    "5. **Interface** : Simple et fonctionnelle\n",
    "\n",
    "## Impl√©mentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de connaissances\n",
    "faq = [\n",
    "    {\"question\": \"Qu'est-ce que l'intelligence artificielle ?\",\n",
    "     \"reponse\": \"L'IA est la simulation de l'intelligence humaine par des machines.\"},\n",
    "    {\"question\": \"Comment apprendre le machine learning ?\",\n",
    "     \"reponse\": \"Commencez par Python, les math√©matiques, puis pratiquez avec des projets.\"},\n",
    "    {\"question\": \"Quels sont les pr√©requis pour ce cours ?\",\n",
    "     \"reponse\": \"Python, bases de math√©matiques et motivation pour apprendre.\"},\n",
    "    {\"question\": \"Combien de temps dure la formation ?\",\n",
    "     \"reponse\": \"La formation compl√®te dure environ 6 mois avec 20h de cours par semaine.\"},\n",
    "    {\"question\": \"Y a-t-il des certificats ?\",\n",
    "     \"reponse\": \"Oui, vous recevez un certificat apr√®s avoir termin√© tous les modules.\"}\n",
    "]\n",
    "\n",
    "# Pr√©parer les questions\n",
    "questions = [item[\"question\"] for item in faq]\n",
    "reponses = [item[\"reponse\"] for item in faq]\n",
    "\n",
    "# Vectoriser\n",
    "vectorizer = TfidfVectorizer()\n",
    "questions_vectorized = vectorizer.fit_transform(questions)\n",
    "\n",
    "def chatbot_intelligent(question_utilisateur):\n",
    "    \"\"\"Trouve la meilleure r√©ponse\"\"\"\n",
    "    # Vectoriser la question\n",
    "    question_vec = vectorizer.transform([question_utilisateur])\n",
    "    \n",
    "    # Calculer similarit√©\n",
    "    similarities = cosine_similarity(question_vec, questions_vectorized)[0]\n",
    "    \n",
    "    # Trouver la meilleure correspondance\n",
    "    best_idx = similarities.argmax()\n",
    "    best_score = similarities[best_idx]\n",
    "    \n",
    "    # Seuil de confiance\n",
    "    if best_score > 0.3:\n",
    "        return reponses[best_idx], best_score\n",
    "    else:\n",
    "        return \"Je ne suis pas s√ªr de comprendre. Pouvez-vous reformuler ?\", 0.0\n",
    "\n",
    "# Test\n",
    "print(\"ü§ñ CHATBOT INTELLIGENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "questions_test = [\n",
    "    \"C'est quoi l'IA ?\",\n",
    "    \"Je veux apprendre le ML, comment faire ?\",\n",
    "    \"Quelle est la dur√©e de la formation ?\",\n",
    "    \"Est-ce que je peux avoir un dipl√¥me ?\"\n",
    "]\n",
    "\n",
    "for q in questions_test:\n",
    "    reponse, score = chatbot_intelligent(q)\n",
    "    print(f\"\\nüë§ : {q}\")\n",
    "    print(f\"ü§ñ : {reponse}\")\n",
    "    print(f\"   (confiance: {score:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercice 5 : Projet final\n",
    "\n",
    "Am√©liorez le chatbot :\n",
    "1. Ajoutez 15 nouvelles questions/r√©ponses\n",
    "2. Am√©liorez le pr√©traitement\n",
    "3. Ajustez le seuil de confiance\n",
    "4. Testez avec vos propres questions\n",
    "5. Cr√©ez une interface simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5 : Projet final - √Ä vous de jouer !\n",
    "\n",
    "# TODO: Cr√©ez votre chatbot am√©lior√©\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì R√©sum√© du module\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **NLP** : D√©finition et applications\n",
    "2. **Pr√©traitement** : Nettoyage, tokenisation, vectorisation\n",
    "3. **Word Embeddings** : Repr√©sentations s√©mantiques\n",
    "4. **Transformers** : BERT, GPT, Hugging Face\n",
    "5. **Applications RDC** : Chatbots, traduction, analyse\n",
    "6. **Projet** : Chatbot √©ducatif intelligent\n",
    "\n",
    "### Comp√©tences acquises\n",
    "\n",
    "- ‚úÖ Pr√©traiter des textes\n",
    "- ‚úÖ Vectoriser avec TF-IDF\n",
    "- ‚úÖ Utiliser word embeddings\n",
    "- ‚úÖ Appliquer des Transformers\n",
    "- ‚úÖ Cr√©er des chatbots\n",
    "- ‚úÖ Analyser des textes\n",
    "\n",
    "### Prochaines √©tapes\n",
    "\n",
    "1. **Approfondir** : Fine-tuning, RAG, LangChain\n",
    "2. **Pratiquer** : Plus de projets\n",
    "3. **D√©ployer** : APIs de chatbots\n",
    "\n",
    "---\n",
    "\n",
    "**F√©licitations ! Module NLP termin√© ! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
